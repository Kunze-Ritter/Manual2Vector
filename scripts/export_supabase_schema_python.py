"""
Export Supabase Schema using Python (no pg_dump required)

This script exports the database schema and minimal seed data from Supabase
using pure Python (asyncpg), without requiring PostgreSQL client tools.

Usage:
    python scripts/export_supabase_schema_python.py

Output:
    - database/seeds/01_schema.sql (DDL only)
    - database/seeds/02_minimal_seed.sql (minimal data for testing)
"""

import os
import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Load environment
load_dotenv(PROJECT_ROOT / '.env.database')

try:
    import asyncpg
except ImportError:
    print("‚ùå asyncpg not installed!")
    print("   Install with: pip install asyncpg")
    sys.exit(1)


async def get_connection():
    """Get connection to Supabase database"""
    supabase_url = os.getenv('SUPABASE_URL')
    db_password = os.getenv('SUPABASE_DB_PASSWORD')
    
    if not supabase_url:
        print("‚ùå Missing SUPABASE_URL in .env.database")
        sys.exit(1)
    
    if not db_password:
        print("‚ùå Missing SUPABASE_DB_PASSWORD in .env.database")
        print("\n   Get it from: Supabase Dashboard > Settings > Database")
        print("   Add to .env.database:")
        print("   SUPABASE_DB_PASSWORD=your-password-here")
        sys.exit(1)
    
    # Extract project ref from URL
    project_ref = supabase_url.replace('https://', '').replace('.supabase.co', '')
    
    # Build connection string
    conn_string = f"postgresql://postgres:{db_password}@db.{project_ref}.supabase.co:5432/postgres"
    
    print(f"üì° Connecting to Supabase project: {project_ref}")
    
    try:
        conn = await asyncpg.connect(conn_string)
        print("‚úÖ Connected successfully")
        return conn
    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        print("\n   Check:")
        print("   1. SUPABASE_DB_PASSWORD is correct")
        print("   2. Your IP is allowed in Supabase (Database Settings > Connection Pooling)")
        print("   3. Database is not paused")
        sys.exit(1)


async def export_schema(conn):
    """Export database schema (DDL)"""
    print("\n" + "="*70)
    print("EXPORTING SCHEMA")
    print("="*70)
    
    output_dir = PROJECT_ROOT / 'database' / 'seeds'
    output_dir.mkdir(parents=True, exist_ok=True)
    schema_file = output_dir / '01_schema.sql'
    
    schemas = ['krai_core', 'krai_content', 'krai_intelligence', 'krai_system', 'krai_parts']
    
    with open(schema_file, 'w', encoding='utf-8') as f:
        f.write("--\n")
        f.write("-- Supabase Database Schema Export\n")
        f.write("-- Generated by: export_supabase_schema_python.py\n")
        f.write("--\n\n")
        
        # Create schemas
        f.write("-- Create schemas\n")
        for schema in schemas:
            f.write(f"CREATE SCHEMA IF NOT EXISTS {schema};\n")
        f.write("\n")
        
        # Export each schema
        for schema in schemas:
            print(f"üì¶ Exporting schema: {schema}")
            
            f.write(f"-- Schema: {schema}\n")
            f.write(f"-- ==========================================\n\n")
            
            # Get all tables in schema
            tables = await conn.fetch("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = $1 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """, schema)
            
            for table_row in tables:
                table_name = table_row['table_name']
                full_table = f"{schema}.{table_name}"
                
                print(f"   - {full_table}")
                
                # Get table DDL
                f.write(f"-- Table: {full_table}\n")
                f.write(f"DROP TABLE IF EXISTS {full_table} CASCADE;\n")
                
                # Get columns
                columns = await conn.fetch("""
                    SELECT 
                        column_name,
                        data_type,
                        udt_name,
                        character_maximum_length,
                        is_nullable,
                        column_default
                    FROM information_schema.columns
                    WHERE table_schema = $1 AND table_name = $2
                    ORDER BY ordinal_position
                """, schema, table_name)
                
                # Build CREATE TABLE
                f.write(f"CREATE TABLE {full_table} (\n")
                
                col_defs = []
                for col in columns:
                    col_name = col['column_name']
                    
                    # Determine data type
                    if col['data_type'] == 'USER-DEFINED':
                        col_type = col['udt_name']
                    elif col['data_type'] == 'character varying':
                        if col['character_maximum_length']:
                            col_type = f"varchar({col['character_maximum_length']})"
                        else:
                            col_type = "varchar"
                    elif col['data_type'] == 'ARRAY':
                        col_type = col['udt_name']
                    else:
                        col_type = col['data_type']
                    
                    # Build column definition
                    col_def = f"    {col_name} {col_type}"
                    
                    # Add NOT NULL
                    if col['is_nullable'] == 'NO':
                        col_def += " NOT NULL"
                    
                    # Add DEFAULT
                    if col['column_default']:
                        col_def += f" DEFAULT {col['column_default']}"
                    
                    col_defs.append(col_def)
                
                f.write(",\n".join(col_defs))
                f.write("\n);\n\n")
                
                # Get indexes
                indexes = await conn.fetch("""
                    SELECT indexname, indexdef
                    FROM pg_indexes
                    WHERE schemaname = $1 AND tablename = $2
                    AND indexname NOT LIKE '%_pkey'
                """, schema, table_name)
                
                for idx in indexes:
                    f.write(f"-- Index: {idx['indexname']}\n")
                    f.write(f"{idx['indexdef']};\n\n")
            
            f.write("\n")
    
    print(f"\n‚úÖ Schema exported to: {schema_file}")
    print(f"   File size: {schema_file.stat().st_size / 1024:.1f} KB")


async def export_seed_data(conn):
    """Export minimal seed data"""
    print("\n" + "="*70)
    print("EXPORTING SEED DATA")
    print("="*70)
    
    output_dir = PROJECT_ROOT / 'database' / 'seeds'
    seed_file = output_dir / '02_minimal_seed.sql'
    
    # Tables to export (with row limits)
    tables_to_export = [
        ('krai_core.manufacturers', 100),
        ('krai_core.product_series', 100),
        ('krai_core.products', 50),
    ]
    
    with open(seed_file, 'w', encoding='utf-8') as f:
        f.write("--\n")
        f.write("-- Minimal Seed Data for Local Development\n")
        f.write("--\n")
        f.write("-- This file contains essential reference data (manufacturers, products, etc.)\n")
        f.write("-- for local testing. It does NOT include:\n")
        f.write("-- - User data\n")
        f.write("-- - Production documents\n")
        f.write("-- - Embeddings\n")
        f.write("-- - Large binary data\n")
        f.write("--\n\n")
        
        for full_table, limit in tables_to_export:
            schema, table = full_table.split('.')
            
            print(f"üì¶ Exporting: {full_table} (limit: {limit})")
            
            # Check if table exists
            exists = await conn.fetchval("""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.tables 
                    WHERE table_schema = $1 AND table_name = $2
                )
            """, schema, table)
            
            if not exists:
                print(f"   ‚ö†Ô∏è  Table does not exist, skipping")
                continue
            
            # Get column names
            columns = await conn.fetch("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = $1 AND table_name = $2
                ORDER BY ordinal_position
            """, schema, table)
            
            col_names = [col['column_name'] for col in columns]
            
            # Get data
            rows = await conn.fetch(f"""
                SELECT * FROM {full_table}
                LIMIT $1
            """, limit)
            
            if not rows:
                print(f"   ‚ÑπÔ∏è  No data found")
                continue
            
            print(f"   ‚úÖ Exported {len(rows)} rows")
            
            f.write(f"-- Table: {full_table}\n")
            f.write(f"-- Rows: {len(rows)}\n\n")
            
            # Write INSERT statements
            for row in rows:
                values = []
                for col_name in col_names:
                    val = row[col_name]
                    if val is None:
                        values.append('NULL')
                    elif isinstance(val, str):
                        # Escape single quotes
                        escaped = val.replace("'", "''")
                        values.append(f"'{escaped}'")
                    elif isinstance(val, (list, dict)):
                        # JSON/Array
                        import json
                        escaped = json.dumps(val).replace("'", "''")
                        values.append(f"'{escaped}'")
                    else:
                        values.append(str(val))
                
                f.write(f"INSERT INTO {full_table} ({', '.join(col_names)}) VALUES ({', '.join(values)});\n")
            
            f.write("\n")
    
    print(f"\n‚úÖ Seed data exported to: {seed_file}")
    print(f"   File size: {seed_file.stat().st_size / 1024:.1f} KB")


async def main():
    """Main export workflow"""
    print("\n" + "="*70)
    print("SUPABASE SCHEMA & SEED EXPORT (Python)")
    print("="*70)
    
    print("\nThis script will export:")
    print("  1. Database schema (DDL) for all krai_* schemas")
    print("  2. Minimal seed data (manufacturers, products, etc.)")
    print("\nNo pg_dump required - uses pure Python!")
    
    # Connect
    conn = await get_connection()
    
    try:
        # Export schema
        await export_schema(conn)
        
        # Export seed data
        await export_seed_data(conn)
        
    finally:
        await conn.close()
    
    print("\n" + "="*70)
    print("‚úÖ EXPORT COMPLETE")
    print("="*70)
    
    print("\nNext steps:")
    print("  1. Review exported files in database/seeds/")
    print("  2. Test with: docker-compose down -v && docker-compose up -d krai-postgres")
    print("  3. Verify with: python scripts/test_adapter_quick.py")
    print("\nüí° Tip: Keep seed files < 1 MB and exclude sensitive data")


if __name__ == '__main__':
    asyncio.run(main())
