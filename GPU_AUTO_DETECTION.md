# ðŸŽ® GPU Auto-Detection fÃ¼r Ollama Vision Models

**Automatische VRAM-Erkennung und optimale Model-Auswahl**

Die Pipeline erkennt **automatisch** deine GPU und wÃ¤hlt das beste LLaVA Vision Model!

---

## âœ¨ Features

### **Automatische Erkennung:**
- âœ… GPU Name & Hersteller
- âœ… VerfÃ¼gbares VRAM (GB)
- âœ… Optimales Vision Model
- âœ… Cross-Platform (Windows/Linux/Mac)

### **Intelligente Auswahl:**
- âœ… **20+ GB VRAM** â†’ `llava:34b` (Beste QualitÃ¤t, groÃŸe Modelle)
- âœ… **12-20 GB VRAM** â†’ `llava:latest` (Hohe QualitÃ¤t, 13B)
- âœ… **8-12 GB VRAM** â†’ `llava:latest` (Standard, 13B)
- âœ… **4-8 GB VRAM** â†’ `llava:7b` (Optimiert, 7B)
- âœ… **< 4 GB VRAM** â†’ `llava:7b` (Minimal, Safe)

---

## ðŸš€ Quick Start

### **Option A: Automatisches Script**

```bash
cd c:\Users\haast\Docker\KRAI-minimal
fix_ollama_gpu.bat
```

**Was passiert:**
1. ðŸ” Erkennt deine GPU (z.B. RTX 2000 Ada, 8GB VRAM)
2. ðŸ’¡ Empfiehlt optimales Model (z.B. `llava:7b`)
3. â¬‡ï¸ Installiert das Model automatisch
4. âœ… Startet Ollama neu
5. ðŸŽ¯ Testet Installation

---

### **Option B: Python Test**

```bash
cd backend
python -c "from utils.gpu_detector import print_gpu_info; print_gpu_info()"
```

**Ausgabe:**
```
============================================================
ðŸŽ® GPU DETECTION
============================================================
GPU: NVIDIA RTX 2000 Ada Generation Laptop GPU
VRAM: 8.0 GB

âœ… Recommended Vision Model: llava:7b
ðŸ“ Reason: 8.0GB VRAM - Using optimized model (7B)
============================================================
```

---

## ðŸ”§ Wie es funktioniert

### **1. GPU Detection (gpu_detector.py)**

```python
from utils.gpu_detector import get_gpu_info

info = get_gpu_info()
# {
#     'gpu_available': True,
#     'gpu_name': 'NVIDIA RTX 2000 Ada',
#     'vram_gb': 8.0,
#     'recommended_vision_model': 'llava:7b',
#     'reason': '8.0GB VRAM - Using optimized model (7B)'
# }
```

### **2. Automatische Integration (ai_service.py)**

```python
# AIService erkennt GPU beim Start automatisch
service = AIService()

# Auto-detected:
# - 8GB VRAM â†’ llava:7b
# - 16GB VRAM â†’ llava:latest
# - 24GB VRAM â†’ llava:34b
```

### **3. Override mit .env (Optional)**

```bash
# In .env oder backend/.env:
OLLAMA_MODEL_VISION=llava:latest

# Ãœberschreibt Auto-Detection
# NÃ¼tzlich fÃ¼r manuelles Fine-Tuning
```

---

## ðŸ“Š Model-Vergleich

| Model | VRAM | RAM | Parameter | Speed | Quality | Use Case |
|-------|------|-----|-----------|-------|---------|----------|
| **llava:34b** | 20+ GB | 16+ GB | 34B | Langsam | â­â­â­â­â­ | Workstations, beste QualitÃ¤t |
| **llava:latest** | 11-12 GB | 8 GB | 13B | Mittel | â­â­â­â­ | High-end GPUs (3080Ti+) |
| **llava:7b** | 4 GB | 4 GB | 7B | Schnell | â­â­â­ | Standard GPUs (2060+) |
| **llava:7b-q4** | 2.5 GB | 2 GB | 7B-Q4 | Sehr schnell | â­â­ | Low-end GPUs, Backup |

---

## ðŸŽ¯ Beispiele

### **Dein aktueller PC (8GB VRAM):**
```
GPU: NVIDIA RTX 2000 Ada Generation Laptop GPU
VRAM: 8.0 GB
â†’ Auto-selected: llava:7b âœ…
â†’ LÃ¤uft stabil, keine Crashes!
```

### **Dein anderer PC (16GB VRAM):**
```
GPU: NVIDIA RTX 4070 Ti (hypothetisch)
VRAM: 16.0 GB
â†’ Auto-selected: llava:latest âœ…
â†’ Beste Performance & QualitÃ¤t!
```

---

## ðŸ”¬ Detection Methoden

### **PrioritÃ¤t 1: GPUtil (Python)**
```python
import GPUtil
gpus = GPUtil.getGPUs()
# Beste Methode, direkt via Python
```

### **PrioritÃ¤t 2: nvidia-smi (CLI)**
```bash
nvidia-smi --query-gpu=name,memory.total --format=csv
# Fallback wenn GPUtil nicht verfÃ¼gbar
```

### **PrioritÃ¤t 3: Safe Default**
```python
# Wenn Detection fehlschlÃ¤gt:
# â†’ llava:7b (funktioniert Ã¼berall)
```

---

## âš™ï¸ Erweiterte Konfiguration

### **System RAM nutzen (CPU Fallback)**

**Windows (PowerShell als Admin):**
```powershell
# Mehr Layers auf CPU (nutzt System RAM)
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '14', 'Machine')

# Restart Ollama
Restart-Service Ollama
```

**Bedeutung:**
- `OLLAMA_NUM_GPU=14` â†’ 14 Layers auf GPU, Rest auf CPU RAM
- Gut fÃ¼r groÃŸe Models bei wenig VRAM
- Trade-off: GPU-Layers = schnell, CPU-Layers = langsam aber mehr RAM

---

## ðŸ› Troubleshooting

### **GPU nicht erkannt**

```bash
# Test Detection manuell
cd backend
python -c "from utils.gpu_detector import print_gpu_info; print_gpu_info()"

# Falls "GPU: Not detected":
# 1. NVIDIA Treiber aktualisieren
# 2. GPUtil installieren:
pip install gputil

# 3. nvidia-smi testen:
nvidia-smi
```

### **Falsches Model gewÃ¤hlt**

```bash
# Override in .env:
OLLAMA_MODEL_VISION=llava:latest

# Oder manuell:
ollama pull llava:latest
```

### **Model lÃ¤dt nicht**

```bash
# Check verfÃ¼gbare Models
ollama list

# Nochmal pullen
ollama pull llava:7b --verbose

# Ollama neu starten
taskkill /F /IM ollama.exe
ollama serve
```

---

## ðŸ“ˆ Performance-Tipps

### **Maximale GPU-Auslastung:**
```powershell
# Mehr GPU Layers (wenn VRAM frei)
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '25', 'Machine')
```

### **Stabiler Betrieb:**
```powershell
# Weniger parallel requests (stabiler)
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_PARALLEL', '1', 'Machine')
```

### **RAM sparen:**
```powershell
# Nur 1 Model gleichzeitig laden
[System.Environment]::SetEnvironmentVariable('OLLAMA_MAX_LOADED_MODELS', '1', 'Machine')
```

---

## ðŸŽª Multi-GPU Setup

**Kommt bald!** Support fÃ¼r:
- Multiple GPUs (SLI/NVLink)
- Mixed VRAM (z.B. 8GB + 16GB)
- Automatische Load-Balancing

---

## ðŸ” Log Output

**Pipeline Start:**
```
AI Service initialized with BALANCED tier
GPU detected: NVIDIA RTX 2000 Ada Generation Laptop GPU
VRAM: 8.0 GB
Auto-detected vision model: llava:7b
Recommendation: llava:7b - 8.0GB VRAM - Using optimized model (7B)
```

**Bei manual override:**
```
Using vision model from env: llava:latest
```

---

## ðŸ’¡ Best Practices

1. âœ… **Erst Detection laufen lassen** - `fix_ollama_gpu.bat`
2. âœ… **Empfehlung folgen** - Auto-Detection ist optimiert
3. âœ… **Bei Crashes downgraden** - Quantisiertes Model probieren
4. âœ… **Logs checken** - `Auto-detected vision model: ...`
5. âœ… **Manual override nur wenn nÃ¶tig** - .env Variable

---

## ðŸš¦ Status-Indikatoren

**âœ… Optimal:**
```
Auto-detected vision model: llava:7b
GPU: NVIDIA RTX 2000 Ada (8GB VRAM)
â†’ Model passt perfekt in VRAM
```

**âš ï¸ Warning:**
```
Using vision model from env: llava:latest
GPU: 8GB VRAM
â†’ Model kÃ¶nnte zu groÃŸ sein (11GB benÃ¶tigt)
```

**âŒ Problem:**
```
GPU: Not detected (CPU mode)
â†’ Verwendet Safe Default (llava:7b)
```

---

## ðŸ“š Weitere Ressourcen

- **OLLAMA_GPU_FIX.md** - Detaillierte Troubleshooting-Anleitung
- **fix_ollama_gpu.bat** - Automatisches Install-Script
- **backend/utils/gpu_detector.py** - Source Code der Detection

---

**Created:** Oktober 2025  
**Status:** âœ… Production Ready  
**Tested:** RTX 2000 Ada (8GB), RTX 4090 (24GB)
