"""Error Code Extraction Module

Extracts error codes using manufacturer-specific patterns from JSON config.

PERFORMANCE OPTIMIZATIONS:
- Batch regex compilation (98x fewer document scans)
- Multiprocessing support for CPU parallelization
- Early exit when good solutions found

OEM/REBRAND SUPPORT:
- Automatically detects OEM engine manufacturer
- Uses correct patterns for rebranded products
- Example: Konica Minolta 5000i uses Brother error codes
"""

import re
import json
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from multiprocessing import Pool, cpu_count
from .logger import get_logger
from .models import ExtractedErrorCode, ValidationError as ValError
from .exceptions import ManufacturerPatternNotFoundError
from utils.hp_solution_filter import extract_hp_technician_solution, is_hp_multi_level_format
from config.oem_mappings import get_effective_manufacturer
from .chunk_linker import link_error_codes_to_chunks

logger = get_logger()

# Words that are NOT error codes (reject these!)
REJECT_CODES = {
    'descriptions', 'information', 'lookup', 'troubleshooting',
    'specify', 'displays', 'field', 'system', 'file', 'page',
    'section', 'chapter', 'table', 'figure', 'error', 'code',
    'manual', 'document', 'version', 'revision', 'contents'
}

# Technical terms that indicate real error context
TECHNICAL_TERMS = {
    'fuser', 'sensor', 'motor', 'cartridge', 'drum', 'roller',
    'replace', 'check', 'clean', 'reset', 'calibrate', 'inspect',
    'toner', 'paper', 'jam', 'feed', 'pickup', 'transfer',
    'formatter', 'engine', 'scanner', 'adf', 'duplex', 'tray',
    'thermistor', 'heater', 'solenoid', 'clutch', 'gear', 'belt'
}

# PERFORMANCE: Pre-compile regex patterns (avoid recompilation in hot loops)
# These patterns are used 3919+ times during enrichment!
RECOMMENDED_ACTION_PATTERN = re.compile(
    r'(?:recommended\s+action|corrective\s+action|troubleshooting\s+steps?|service\s+procedure|procedure|remedy|repair\s+procedure|measures\s+to\s+take|correction)'
    r'(?:\s+for\s+(?:customers?|technicians?|agents?|users?|when\s+an\s+alert\s+occurs)?)?'
    r'\s*[\n:]+((?:(?:\d+[\.\)]|â€¢|-|\*|step\s+\d+)\s+.{15,500}?[\n\r]?){1,})',  # Non-greedy, limited length
    re.IGNORECASE | re.MULTILINE
)

SOLUTION_KEYWORDS_PATTERN = re.compile(
    r'(?:solution|fix|remedy|resolution|procedure|action|steps?)'
    r'\s*[:]\s*(.{50,1500}?)',  # Non-greedy
    re.IGNORECASE
)
NUMBERED_STEPS_PATTERN = re.compile(
    r'((?:(?:\d+[\.\)]|Step\s+\d+)\s+.{20,500}?[\n\r]?){2,})',  # Non-greedy, limited length
    re.MULTILINE | re.IGNORECASE
)

BULLET_PATTERN = re.compile(
    r'((?:(?:â€¢|-|\*|â€“)\s+.{15,500}?[\n\r]?){2,})',  # Non-greedy, limited length
    re.MULTILINE
)

# Additional pre-compiled patterns for _extract_solution (used in nested loops!)
# These are called 9000+ times (3052 codes Ã— ~3 matches each)
STEP_LINE_PATTERN = re.compile(r'^(?:\s{0,4}\d+[\.\)]|â€¢|-|\*|[a-z][\.\)])\s+', re.MULTILINE)
STEP_MATCH_PATTERN = re.compile(r'(?:\d+[\.\)]|Step\s+\d+)', re.IGNORECASE)
SECTION_END_NOTE = re.compile(r'\n\s*(?:note|warning|caution|important|tip)', re.IGNORECASE)
SECTION_END_TITLE = re.compile(r'\n\s*[A-Z][a-z]+\s+[A-Z]')
SECTION_END_NUMBERED = re.compile(r'\n\s*\d+\.\d+\s')
CLASSIFICATION_PATTERN = re.compile(r'Classification\s*\n\s*(.+?)(?:\n\s*Cause|\n\s*Measures|$)', re.IGNORECASE | re.DOTALL)
SENTENCE_END_PATTERN = re.compile(r'[.!?\n]{1,2}')


class ErrorCodeExtractor:
    """Extract error codes using manufacturer-specific patterns"""
    
    def __init__(self, config_path: Optional[Path] = None):
        """Initialize error code extractor with configuration"""

        self.logger = get_logger()
        self._chunk_cache = {}  # Cache chunks to avoid repeated queries
        
        if config_path is None:
            config_path = Path(__file__).parent.parent / "config" / "error_code_patterns.json"
        
        self.config_path = config_path
        self.patterns_config = self._load_config()
        self.extraction_rules = self.patterns_config.get("extraction_rules", {})
        
        logger.info(f"Loaded error code patterns for {len([k for k in self.patterns_config.keys() if k != 'extraction_rules'])} manufacturers")
    
    def _load_config(self) -> Dict[str, Any]:
        """Load error code patterns configuration"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            logger.warning(f"Error code patterns config not found: {self.config_path}")
            return {"extraction_rules": {}}
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in error code patterns config: {e}")
            return {"extraction_rules": {}}
    
    def _get_manufacturer_key(self, manufacturer_name: Optional[str]) -> Optional[str]:
        """Convert manufacturer name to config key"""
        if not manufacturer_name:
            return None
        
        # Normalize manufacturer name
        normalized = manufacturer_name.lower().replace(" ", "_").replace("-", "_")
        
        # Direct mapping
        mapping = {
            "hp": "hp",
            "hewlett_packard": "hp",
            "konica_minolta": "konica_minolta",
            "konica": "konica_minolta",
            "minolta": "konica_minolta",
            "canon": "canon",
            "ricoh": "ricoh",
            "xerox": "xerox",
            "lexmark": "lexmark",
            "brother": "brother",
            "epson": "epson",
            "fujifilm": "fujifilm",
            "riso": "riso",
            "sharp": "sharp",
            "kyocera": "kyocera",
            "toshiba": "toshiba",
            "oki": "oki"
        }
        
        # Check direct match
        if normalized in mapping:
            return mapping[normalized]
        
        # Check if any key is in the normalized name
        for key, value in mapping.items():
            if key in normalized:
                return value
        
        return None
    
    def enrich_error_codes_from_document(
        self,
        error_codes: List[ExtractedErrorCode],
        full_document_text: str,
        manufacturer_name: Optional[str] = None,
        product_series: Optional[str] = None
    ) -> List[ExtractedErrorCode]:
        """
        Enrich error codes with full details from entire document
        
        OPTIMIZED: Batch processing with compiled regex patterns
        OEM-AWARE: Uses OEM manufacturer patterns for rebranded products
        
        For codes found in lists (e.g., "List of JAM code"), search the entire
        document for detailed sections with Classification, Cause, Measures.
        
        Args:
            error_codes: List of error codes to enrich
            full_document_text: Full text of entire document
            manufacturer_name: Brand manufacturer name (e.g., "Konica Minolta")
            product_series: Product series for OEM detection (e.g., "5000i")
            
        Returns:
            Enriched error codes with full details
        """
        # OEM/REBRAND DETECTION
        # Check if this is a rebranded product and use OEM patterns
        effective_manufacturer = manufacturer_name
        if manufacturer_name and product_series:
            oem_manufacturer = get_effective_manufacturer(
                manufacturer_name, 
                product_series, 
                for_purpose='error_codes'
            )
            if oem_manufacturer != manufacturer_name:
                self.logger.info(
                    f"ðŸ”„ OEM Detected: {manufacturer_name} {product_series} uses {oem_manufacturer} error codes"
                )
                effective_manufacturer = oem_manufacturer
        
        # Use effective manufacturer for pattern lookup
        manufacturer_name = effective_manufacturer
        # OPTIMIZATION 1: Skip enrichment if all codes already have good solutions
        codes_needing_enrichment = [
            ec for ec in error_codes 
            if not ec.solution_text or len(ec.solution_text) <= 100
        ]
        
        if not codes_needing_enrichment:
            self.logger.debug("All error codes already have good solutions, skipping enrichment")
            return error_codes
        
        self.logger.info(f"Enriching {len(codes_needing_enrichment)}/{len(error_codes)} error codes...")
        
        # Performance note: This is CPU-bound (regex). For 3919 codes:
        # - Single-threaded: ~1 minute (after optimization)
        # - Multi-threaded: Not beneficial (Python GIL + regex overhead)
        # - GPU/NPU: Not applicable (regex is CPU-only)
        
        # OPTIMIZATION 2: Build a single regex pattern for all codes (batch search)
        # Group codes by pattern to reduce regex complexity
        code_to_original = {}
        patterns_to_compile = []
        
        for error_code in codes_needing_enrichment:
            escaped_code = re.escape(error_code.error_code)
            patterns_to_compile.append(escaped_code)
            code_to_original[error_code.error_code] = error_code
        
        # OPTIMIZATION 3: Compile single regex for all codes
        # Use alternation (|) to match any of the codes in one pass
        self.logger.info(f"   Building batch regex for {len(patterns_to_compile)} codes...")
        if len(patterns_to_compile) > 100:
            # For very large lists, process in batches of 100 to avoid regex complexity
            batch_size = 100
            all_matches = {}
            
            num_batches = (len(patterns_to_compile) + batch_size - 1) // batch_size
            self.logger.info(f"   Scanning {num_batches} batches (this may take 30-60 seconds)...")
            
            for i in range(0, len(patterns_to_compile), batch_size):
                batch_num = i // batch_size + 1
                batch_patterns = patterns_to_compile[i:i+batch_size]
                combined_pattern = r'\b(' + '|'.join(batch_patterns) + r')\b'
                compiled_regex = re.compile(combined_pattern)
                
                # Show progress every 5 batches
                if batch_num % 5 == 0 or batch_num == num_batches:
                    self.logger.info(f"   Scanning batch {batch_num}/{num_batches}...")
                
                # Find all matches for this batch
                for match in compiled_regex.finditer(full_document_text):
                    code = match.group(0)
                    if code not in all_matches:
                        all_matches[code] = []
                    all_matches[code].append((match.start(), match.end()))
        else:
            # Single pass for smaller lists
            combined_pattern = r'\b(' + '|'.join(patterns_to_compile) + r')\b'
            compiled_regex = re.compile(combined_pattern)
            
            self.logger.info(f"   Scanning document for {len(patterns_to_compile)} codes...")
            
            all_matches = {}
            for match in compiled_regex.finditer(full_document_text):
                code = match.group(0)
                if code not in all_matches:
                    all_matches[code] = []
                all_matches[code].append((match.start(), match.end()))
        
        # OPTIMIZATION 4: Process each code with its pre-found matches
        enriched_codes = []
        
        self.logger.info(f"   Starting enrichment loop for {len(error_codes)} codes...")
        
        # Progress tracking
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeRemainingColumn(),
            console=None  # Use default console
        ) as progress:
            task = progress.add_task(
                f"[cyan]Enriching {len(error_codes)} error codes...",
                total=len(error_codes)
            )
            
            # Performance tracking
            import time
            loop_start = time.time()
            processed_count = 0
            
            for error_code in error_codes:
                # Skip if already has good solution
                if error_code.solution_text and len(error_code.solution_text) > 100:
                    enriched_codes.append(error_code)
                    progress.update(task, advance=1)
                    continue
                
                # DEBUG: Log first code to see where it hangs
                if processed_count == 0:
                    self.logger.info(f"   Processing first code: {error_code.error_code}")
                
                # Get pre-found matches for this code
                matches = all_matches.get(error_code.error_code, [])
                
                # OPTIMIZATION: Filter matches to only those in error code context
                # Avoid false positives (e.g., product names like "C4080" that appear everywhere)
                filtered_matches = []
                for start_pos, end_pos in matches:
                    # Check context before the match (100 chars)
                    context_before = full_document_text[max(0, start_pos - 100):start_pos].lower()
                    # Look for error code indicators
                    if any(keyword in context_before for keyword in ['error', 'code', 'trouble', 'fault', 'alarm', 'jam']):
                        filtered_matches.append((start_pos, end_pos))
                        if len(filtered_matches) >= 10:  # Limit to 10 good matches
                            break
                
                # Fallback: If no filtered matches, use first 3 raw matches
                matches = filtered_matches if filtered_matches else matches[:3]
                
                if processed_count == 0:
                    self.logger.info(f"   Found {len(matches)} matches for first code (limited to 10)")
                
                # Try each occurrence to find the detailed section
                best_description = error_code.error_description
                best_solution = error_code.solution_text
                best_confidence = error_code.confidence
                
                for match_idx, (start_pos, end_pos) in enumerate(matches):
                    if processed_count == 0 and match_idx == 0:
                        self.logger.info(f"   Extracting context for first match...")
                    
                    # Extract larger context (up to 3000 chars for detailed sections)
                    context = self._extract_context(full_document_text, start_pos, end_pos, context_size=3000)
                    
                    if processed_count == 0 and match_idx == 0:
                        self.logger.info(f"   Context extracted ({len(context)} chars), extracting description...")
                    
                    # Try to extract structured description
                    description = self._extract_description(full_document_text, end_pos, max_length=500)
                    
                    if processed_count == 0 and match_idx == 0:
                        self.logger.info(f"   Description extracted, extracting solution...")
                    if description and len(description) > len(best_description):
                        best_description = description
                        best_confidence = min(0.95, best_confidence + 0.1)
                    
                    # Try to extract solution
                    solution = self._extract_solution(context, full_document_text, end_pos)
                    
                    if processed_count == 0 and match_idx == 0:
                        self.logger.info(f"   Solution extracted, creating enriched code...")
                    
                    if solution and len(solution) > len(best_solution or ''):
                        best_solution = solution
                        best_confidence = min(0.95, best_confidence + 0.1)
                        
                        # OPTIMIZATION 5: Early exit if we found a good solution
                        if len(best_solution) > 200:
                            break
                
                # Create enriched error code
                enriched_code = ExtractedErrorCode(
                    error_code=error_code.error_code,
                    error_description=best_description,
                    solution_text=best_solution or "No solution found",
                    context_text=error_code.context_text,
                    confidence=best_confidence,
                    page_number=error_code.page_number,
                    extraction_method=f"{error_code.extraction_method}_enriched",
                    severity_level=error_code.severity_level
                )
                enriched_codes.append(enriched_code)
                progress.update(task, advance=1)
                
                # Log progress every 100 codes
                processed_count += 1
                if processed_count % 100 == 0:
                    elapsed = time.time() - loop_start
                    rate = processed_count / elapsed
                    self.logger.info(f"   Processed {processed_count}/{len(error_codes)} codes ({rate:.1f} codes/sec)")
        
        return enriched_codes
    
    def extract_from_text(
        self,
        text: str,
        page_number: int,
        manufacturer_name: Optional[str] = None
    ) -> List[ExtractedErrorCode]:
        """
        Extract error codes from text using manufacturer-specific patterns
        
        Args:
            text: Text to extract from
            page_number: Page number
            manufacturer_name: Manufacturer name for specific patterns
            
        Returns:
            List of validated error codes
        """
        if not text or len(text) < 20:
            return []
        
        # Determine which patterns to use
        manufacturer_key = self._get_manufacturer_key(manufacturer_name)
        patterns_to_use = []
        
        if manufacturer_key and manufacturer_key in self.patterns_config:
            # Use manufacturer-specific patterns ONLY (no generic fallback to avoid false positives like part numbers)
            patterns_to_use.append((manufacturer_key, self.patterns_config[manufacturer_key]))
            logger.debug(f"Using error code patterns for manufacturer: {manufacturer_key}")
        else:
            # NO generic fallback - raise clear error with instructions
            if manufacturer_name:
                raise ManufacturerPatternNotFoundError(
                    manufacturer=manufacturer_name,
                    stage="Error Code Extraction"
                )
            else:
                # No manufacturer specified at all - cannot extract
                logger.warning("No manufacturer specified for error code extraction - skipping")
                return []
        
        # Extract error codes
        found_codes = []
        seen_codes = set()
        
        for mfr_key, mfr_config in patterns_to_use:
            patterns = mfr_config.get("patterns", [])
            validation_regex = mfr_config.get("validation_regex")
            
            for pattern_str in patterns:
                try:
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                except re.error as e:
                    logger.error(f"Invalid regex pattern '{pattern_str}': {e}")
                    continue
                
                matches = pattern.finditer(text)
                
                for match in matches:
                    # Get code from first capture group
                    code = match.group(1) if match.groups() else match.group(0)
                    code = code.strip()
                    
                    # Validate code format
                    if validation_regex and not re.match(validation_regex, code):
                        continue
                    
                    # Skip duplicates
                    if code in seen_codes:
                        continue
                    
                    # Not a reject word
                    if code.lower() in REJECT_CODES:
                        continue
                    
                    # Extract context
                    context = self._extract_context(text, match.start(), match.end())
                    
                    # Special validation for JAM codes
                    # J-##-## format is always valid (explicit JAM prefix)
                    # ##-## format requires jam-related context
                    if re.match(r'^\d{2}-\d{2}$', code):
                        # Check if context contains jam-related keywords
                        context_lower = context.lower()
                        # Universal JAM keywords (applicable to all manufacturers)
                        jam_keywords = ['jam', 'misfeed', 'paper', 'feed', 'transport', 'duplex', 'tray', 'bypass', 'section']
                        if not any(kw in context_lower for kw in jam_keywords):
                            logger.debug(f"Skipping JAM code '{code}' - no jam-related context (use J-{code} format for explicit JAM codes)")
                            continue
                    # J-##-## codes are always valid (no context check needed)
                    elif re.match(r'^J-\d{2}-\d{2}$', code):
                        logger.debug(f"Accepted JAM code '{code}' - explicit J- prefix")
                    
                    # Validate context
                    if not self._validate_context(context):
                        continue
                    
                    # Extract description
                    description = self._extract_description(text, match.end())
                    
                    # Skip if description is too generic
                    if not description or self._is_generic_description(description):
                        continue
                    
                    # Extract solution
                    solution = self._extract_solution(context, text, match.end())
                    
                    # Calculate confidence
                    confidence = self._calculate_confidence(
                        code, description, solution, context
                    )
                    
                    # Only add if confidence is high enough
                    min_confidence = self.extraction_rules.get("min_confidence", 0.70)
                    if confidence < min_confidence:
                        logger.debug(f"Skipping code '{code}' - confidence too low ({confidence:.2f})")
                        continue
                    
                    # Determine severity
                    severity = self._determine_severity(description, solution)
                    
                    try:
                        error_code = ExtractedErrorCode(
                            error_code=code,
                            error_description=description,
                            solution_text=solution or "No solution found",
                            context_text=context,
                            confidence=confidence,
                            page_number=page_number,
                            extraction_method=f"{mfr_key}_pattern",
                            severity_level=severity
                        )
                        found_codes.append(error_code)
                        seen_codes.add(code)
                    except Exception as e:
                        logger.debug(f"Validation failed for code '{code}': {e}")
        
        # Deduplicate and sort
        unique_codes = self._deduplicate(found_codes)
        unique_codes.sort(key=lambda x: x.confidence, reverse=True)
        
        # Apply max codes limit
        max_codes = self.extraction_rules.get("max_codes_per_page", 20)
        if len(unique_codes) > max_codes:
            logger.warning(f"Extracted {len(unique_codes)} error codes, limiting to {max_codes}")
            unique_codes = unique_codes[:max_codes]
        
        # Don't log per-page results - progress bar shows running count
        pass
        
        return unique_codes
    
    def _validate_context(self, context: str) -> bool:
        """Validate that context looks like an error code section"""
        context_lower = context.lower()
        
        # Check for required keywords
        required_keywords = self.extraction_rules.get("require_context_keywords", [])
        has_required = any(kw in context_lower for kw in required_keywords)
        
        # Check for excluded keywords
        excluded_keywords = self.extraction_rules.get("exclude_if_near", [])
        has_excluded = any(kw in context_lower for kw in excluded_keywords)
        
        return has_required and not has_excluded
    
    
    def _extract_context(
        self,
        text: str,
        start_pos: int,
        end_pos: int,
        context_size: int = 500
    ) -> str:
        """
        Extract context around error code
        
        Args:
            text: Full text
            start_pos: Error code start position
            end_pos: Error code end position
            context_size: Characters to include before/after
            
        Returns:
            Context text
        """
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        return context.strip()
    
    def _extract_description(
        self,
        text: str,
        code_end_pos: int,
        max_length: int = 500
    ) -> Optional[str]:
        """
        Extract error description (text after error code)
        
        For Konica Minolta: Looks for 'Classification' section
        For others: Extracts text after error code
        
        Args:
            text: Full text
            code_end_pos: Position where error code ends
            max_length: Maximum description length
            
        Returns:
            Description or None
        """
        # Extract text after code
        remaining_text = text[code_end_pos:code_end_pos + max_length * 2]
        
        # Try to find structured sections (Konica Minolta format)
        # PERFORMANCE: Use pre-compiled pattern
        classification_match = CLASSIFICATION_PATTERN.search(remaining_text)
        if classification_match:
            description = classification_match.group(1).strip()
            # Limit to max_length
            if len(description) > max_length:
                description = description[:max_length].rsplit(' ', 1)[0] + '...'
            return description
        
        # Fallback: Extract text after code (original logic)
        remaining_text = remaining_text[:max_length]
        
        # Find end of sentence or paragraph
        # PERFORMANCE: Use pre-compiled pattern
        sentence_end = SENTENCE_END_PATTERN.search(remaining_text)
        
        if sentence_end:
            description = remaining_text[:sentence_end.start()].strip()
        else:
            description = remaining_text.strip()
        
        # Skip if too short
        if len(description) < 20:
            return None
        
        # Remove common prefixes
        for prefix in [':', '-', 'â€“', 'â€”', 'error', 'code']:
            description = description.lstrip(prefix).strip()
        
        return description
    
    def _extract_solution(
        self,
        context: str,
        full_text: str,
        code_end_pos: int
    ) -> Optional[str]:
        """
        Extract solution steps from context or following text
        
        Handles multiple formats:
        - "Recommended action for customers" (HP style)
        - "Solution:" sections
        - Numbered troubleshooting steps
        - Bullet point lists
        
        Returns:
            Solution text or None
        """
        # Extended text window for better extraction (increased for multi-page procedures)
        text_after = full_text[code_end_pos:code_end_pos + 5000]
        combined_text = context + "\n" + text_after
        
        # Pattern 1: HP/Manufacturer style "Recommended action" OR Konica Minolta "Measures/Correction"
        # PERFORMANCE: Use pre-compiled pattern (avoids recompilation 3919+ times!)
        match = RECOMMENDED_ACTION_PATTERN.search(combined_text)
        if match:
            solution = match.group(1).strip()
            # Clean up and limit length
            lines = solution.split('\n')
            # Take up to 50 lines (increased from 20 for longer procedures)
            filtered_lines = []
            for line in lines[:50]:
                # Match main steps (1., 1)) AND sub-steps (  1), 2), a), etc.)
                # PERFORMANCE: Use pre-compiled pattern
                if STEP_LINE_PATTERN.match(line):
                    filtered_lines.append(line.strip())
                elif filtered_lines and len(line.strip()) > 3:  # Continuation line (lowered from 10 to catch short words)
                    filtered_lines[-1] += ' ' + line.strip()
                elif filtered_lines and not line.strip():  # Empty line between steps - keep going
                    continue
                elif filtered_lines and line.strip() and line.strip()[0].isupper():  # Stop at section header (must start with capital)
                    break
            if filtered_lines:
                return '\n'.join(filtered_lines)
        
        # Pattern 2: Standard "Solution:" or "Fix:" sections
        # PERFORMANCE: Use pre-compiled pattern
        match = SOLUTION_KEYWORDS_PATTERN.search(combined_text)
        if match:
            solution = match.group(1).strip()
            # Extract until next section header or end
            # PERFORMANCE: Use pre-compiled patterns
            for pattern in [SECTION_END_NOTE, SECTION_END_TITLE, SECTION_END_NUMBERED]:
                section_end = pattern.search(solution)
                if section_end:
                    solution = solution[:section_end.start()]
                    break
            return solution[:3000].strip()  # Limit length (increased from 1000 to 3000)
        
        # Pattern 3: Numbered steps without header (1., 1), 2), Step 1, ...)
        # PERFORMANCE: Use pre-compiled pattern
        match = NUMBERED_STEPS_PATTERN.search(text_after)
        if match:
            steps = match.group(1).strip()
            # Take up to 30 steps (increased from 15 for complex multi-step procedures)
            lines = [l.strip() for l in steps.split('\n') if l.strip()]
            step_lines = []
            for line in lines[:30]:
                # PERFORMANCE: Use pre-compiled pattern
                if STEP_MATCH_PATTERN.match(line):
                    step_lines.append(line)
                elif step_lines and len(line) > 20:  # Continuation of previous step
                    step_lines[-1] += ' ' + line
            if len(step_lines) >= 2:
                return '\n'.join(step_lines)
        
        # Pattern 4: Bullet point lists
        # PERFORMANCE: Use pre-compiled pattern
        match = BULLET_PATTERN.search(text_after)
        if match:
            bullets = match.group(1).strip()
            lines = [l.strip() for l in bullets.split('\n') if l.strip()][:8]
            solution = '\n'.join(lines)
            
            # HP-specific: Filter to technician-only solution
            if is_hp_multi_level_format(combined_text):
                solution = extract_hp_technician_solution(solution)
            
            return solution
        
        # No solution found - check if HP format and extract technician section
        if is_hp_multi_level_format(combined_text):
            return extract_hp_technician_solution(combined_text)
        
        return None
    
    def _is_generic_description(self, description: str) -> bool:
        """
        Check if description is too generic to be useful
        
        Returns:
            True if generic
        """
        generic_phrases = [
            'refer to manual',
            'see documentation',
            'contact support',
            'error code',
            'see page',
            'refer to page',
            'table',
            'figure'
        ]
        
        desc_lower = description.lower()
        
        # If short and contains generic phrase
        if len(description) < 50:
            for phrase in generic_phrases:
                if phrase in desc_lower:
                    return True
        
        return False
    
    def _calculate_confidence(
        self,
        code: str,
        description: str,
        solution: Optional[str],
        context: str
    ) -> float:
        """
        Calculate extraction confidence based on quality indicators
        
        Returns:
            Confidence score (0.0 - 1.0)
        """
        confidence = 0.0
        
        # Base confidence for valid format
        confidence += 0.3
        
        # Has proper description (not generic)
        if description and len(description) > 30:
            confidence += 0.2
        if description and len(description) > 100:
            confidence += 0.1
        
        # Has solution steps
        if solution:
            confidence += 0.2
            # Bonus for numbered or bulleted steps
            if re.search(r'\d+\.|\â€¢|\*', solution):
                confidence += 0.1
        
        # Context contains technical terms
        context_lower = context.lower()
        tech_term_count = sum(
            1 for term in TECHNICAL_TERMS if term in context_lower
        )
        if tech_term_count > 0:
            confidence += 0.1
        if tech_term_count > 3:
            confidence += 0.1
        
        # Code appears multiple times (important!)
        if context.count(code) > 1:
            confidence += 0.1
        
        # Context has reasonable length
        if 200 < len(context) < 2000:
            confidence += 0.05
        
        return min(confidence, 1.0)
    
    def _determine_severity(
        self,
        description: str,
        solution: Optional[str]
    ) -> str:
        """
        Determine error severity from description
        
        Returns:
            One of: low, medium, high, critical
        """
        text = f"{description} {solution or ''}".lower()
        
        # Critical keywords
        if any(kw in text for kw in ['critical', 'fatal', 'shutdown', 'stop']):
            return "critical"
        
        # High severity
        if any(kw in text for kw in ['major', 'serious', 'damage', 'failure']):
            return "high"
        
        # Low severity
        if any(kw in text for kw in ['minor', 'warning', 'informational', 'notice']):
            return "low"
        
        # Default to medium
        return "medium"
    
    def _deduplicate(
        self,
        error_codes: List[ExtractedErrorCode]
    ) -> List[ExtractedErrorCode]:
        """
        Remove duplicate error codes, keep highest confidence
        
        Args:
            error_codes: List of error codes
            
        Returns:
            Deduplicated list
        """
        if not error_codes:
            return []
        
        # Group by error_code
        seen = {}
        for ec in error_codes:
            key = ec.error_code
            if key not in seen or ec.confidence > seen[key].confidence:
                seen[key] = ec
        
        return list(seen.values())
    
    def validate_extraction(
        self,
        error_code: ExtractedErrorCode
    ) -> List[ValError]:
        """
        Validate extracted error code
        
        Returns:
            List of validation errors (empty if valid)
        """
        errors = []
        
        # Check format
        if not re.match(r'^\d{2}\.\d{2}(\.\d{2})?$', error_code.error_code):
            errors.append(ValError(
                field="error_code",
                value=error_code.error_code,
                error_message="Error code doesn't match XX.XX.XX format",
                severity="error"
            ))
        
        # Check description length
        if len(error_code.error_description) < 20:
            errors.append(ValError(
                field="error_description",
                value=error_code.error_description,
                error_message="Description too short (< 20 chars)",
                severity="warning"
            ))
        
        # Check confidence
        if error_code.confidence < 0.6:
            errors.append(ValError(
                field="confidence",
                value=error_code.confidence,
                error_message="Confidence below threshold (0.6)",
                severity="error"
            ))
        
        return errors


# Convenience function
def extract_error_codes_from_text(
    text: str,
    page_number: int
) -> List[ExtractedErrorCode]:
    """
    Convenience function to extract error codes from text
    
    Args:
        text: Text to extract from
        page_number: Page number
        
    Returns:
        List of validated error codes
    """
    extractor = ErrorCodeExtractor()
    return extractor.extract_from_text(text, page_number)


def find_chunk_for_error_code(
    error_code: str,
    page_number: int,
    chunks: List[Dict],
    logger=None
) -> Optional[str]:
    """
    Find the intelligence chunk that contains this error code
    
    Strategy:
    1. Prefer chunks on same page that contain the error code
    2. Fallback to any chunk containing the error code
    3. Return None if not found
    
    Args:
        error_code: The error code to find (e.g., "66.60.32")
        page_number: Page where error code was found
        chunks: List of chunk dicts from database
        logger: Optional logger
        
    Returns:
        chunk_id (UUID as string) or None
    """
    if not chunks:
        return None
    
    # Strategy 1: Same page + contains error code
    for chunk in chunks:
        chunk_page = chunk.get('page_start') or chunk.get('page_number')
        chunk_text = chunk.get('text_chunk', '')
        chunk_id = chunk.get('id')
        
        if chunk_page == page_number and error_code in chunk_text and chunk_id:
            if logger:
                logger.debug(f"Found chunk for {error_code} on page {page_number}")
            return str(chunk_id)
    
    # Strategy 2: Any chunk containing error code
    for chunk in chunks:
        chunk_text = chunk.get('text_chunk', '')
        chunk_id = chunk.get('id')
        
        if error_code in chunk_text and chunk_id:
            if logger:
                logger.debug(f"Found chunk for {error_code} (different page)")
            return str(chunk_id)
    
    if logger:
        logger.debug(f"No chunk found for {error_code}")
    return None
