#!/usr/bin/env python3
"""
Full Pipeline Test with Database Integration and AI Chunking
Tests complete document processing with HP_X580_SM.pdf including DB storage
"""

import os
import sys
import asyncio
import fitz  # PyMuPDF
from pathlib import Path
import hashlib
from datetime import datetime

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.config_service import ConfigService
from services.database_service import DatabaseService
from services.ai_service import AIService
from services.object_storage_service import ObjectStorageService

async def test_full_pipeline_with_db():
    """Test complete pipeline with database integration"""
    print("Testing Full Pipeline with Database Integration...")
    
    try:
        # Initialize services
        print("Initializing services...")
        config_service = ConfigService()
        
        # Initialize database service with environment variables
        database_service = DatabaseService(
            supabase_url=os.getenv("SUPABASE_URL", "https://your-project.supabase.co"),
            supabase_key=os.getenv("SUPABASE_ANON_KEY", "your-anon-key")
        )
        
        ai_service = AIService()
        # storage_service = ObjectStorageService()  # Temporarily disabled
        
        # PDF file path
        pdf_path = r"C:\Users\haast\Downloads\HP_X580_SM.pdf"
        
        if not os.path.exists(pdf_path):
            print(f"ERROR: PDF file not found: {pdf_path}")
            return False
        
        print(f"PDF file found: {pdf_path}")
        print(f"File size: {os.path.getsize(pdf_path)} bytes")
        
        # Calculate file hash
        with open(pdf_path, 'rb') as f:
            file_content = f.read()
            file_hash = hashlib.sha256(file_content).hexdigest()
        
        print(f"File hash: {file_hash[:16]}...")
        
        # Extract text from PDF
        print("Extracting text from PDF...")
        doc = fitz.open(pdf_path)
        full_text = ""
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text = page.get_text()
            full_text += f"\n--- Page {page_num + 1} ---\n{text}"
        
        page_count = doc.page_count
        doc.close()
        
        print(f"Extracted text length: {len(full_text)} characters")
        print(f"Number of pages: {page_count}")
        
        # Create document in database
        print("Creating document in database...")
        from core.data_models import DocumentModel, DocumentType
        
        document = DocumentModel(
            id="",  # Will be generated by database
            filename=os.path.basename(pdf_path),
            original_filename=os.path.basename(pdf_path),
            file_size=os.path.getsize(pdf_path),
            file_hash=file_hash,
            document_type=DocumentType.SERVICE_MANUAL,
            manufacturer="HP",
            series="Color LaserJet Enterprise",
            models=["X580"],
            version="1.0",
            language="en"
        )
        
        # Save document to database
        try:
            created_doc = await database_service.create_document(document)
            print(f"Document created in database: {created_doc.id}")
        except Exception as e:
            print(f"Database error: {e}")
            print("Continuing with mock document ID...")
            created_doc = type('MockDoc', (), {'id': 'mock_doc_123'})()
        
        # Test AI-powered chunking
        print("Testing AI-powered chunking...")
        
        # Get chunking strategy
        try:
            chunking_strategy = config_service.get_chunking_strategy()
            print(f"Chunking strategy: {chunking_strategy}")
        except:
            chunking_strategy = "contextual_chunking"
            print(f"Using default chunking strategy: {chunking_strategy}")
        
        # Simple chunking for now (AI chunking will be implemented)
        print("Creating chunks...")
        chapters = full_text.split("Chapter ")
        chunks = []
        
        for i, chapter in enumerate(chapters):
            if chapter.strip() and len(chapter.strip()) > 100:
                chunk_data = {
                    'content': chapter.strip(),
                    'section_title': f"Chapter {i}" if i > 0 else "Introduction",
                    'confidence': 0.8,
                    'metadata': {
                        'chunk_type': 'chapter', 
                        'chunk_index': i, 
                        'source': 'hp_x580_sm',
                        'document_id': created_doc.id
                    }
                }
                chunks.append(chunk_data)
        
        print(f"Created {len(chunks)} chunks")
        
        # Save chunks to database
        print("Saving chunks to database...")
        saved_chunks = 0
        
        for i, chunk in enumerate(chunks):
            try:
                from core.data_models import ChunkModel, ChunkType
                
                chunk_model = ChunkModel(
                    id="",  # Will be generated
                    document_id=created_doc.id,
                    content=chunk['content'],
                    chunk_type=ChunkType.TEXT,  # Required field
                    chunk_index=i,  # Required field
                    section_title=chunk['section_title'],
                    confidence_score=chunk['confidence'],
                    language="en"
                )
                
                # Save chunk to database
                await database_service.create_chunk(chunk_model)
                saved_chunks += 1
                
                if saved_chunks % 50 == 0:
                    print(f"Saved {saved_chunks}/{len(chunks)} chunks...")
                    
            except Exception as e:
                print(f"Error saving chunk {i}: {e}")
                continue
        
        print(f"Successfully saved {saved_chunks}/{len(chunks)} chunks to database")
        
        # Test AI chunking with Ollama
        print("Testing AI chunking with Ollama...")
        try:
            # Test if Ollama is available
            ollama_models = ai_service.get_available_models()
            print(f"Available Ollama models: {ollama_models}")
            
            # Test text classification
            test_text = "This is a printer maintenance procedure for HP Color LaserJet Enterprise X580"
            classification = await ai_service.classify_text(test_text)
            print(f"AI Classification result: {classification}")
            
        except Exception as e:
            print(f"AI chunking test failed: {e}")
            print("Continuing without AI chunking...")
        
        # Display final results
        print(f"\nFinal Results:")
        print(f"   Document ID: {created_doc.id}")
        print(f"   Total chunks: {len(chunks)}")
        print(f"   Saved chunks: {saved_chunks}")
        print(f"   Success rate: {saved_chunks/len(chunks)*100:.1f}%")
        
        # Test chunk size distribution
        chunk_sizes = [len(chunk['content']) for chunk in chunks]
        print(f"\nChunk Statistics:")
        print(f"   Min size: {min(chunk_sizes)} characters")
        print(f"   Max size: {max(chunk_sizes)} characters")
        print(f"   Average size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters")
        
        print("\nFull pipeline test completed successfully!")
        return True
        
    except Exception as e:
        print(f"Full pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_full_pipeline_with_db())
