#!/usr/bin/env python3
"""
Real Database Integration Test with Supabase
Tests complete document processing with real Supabase database
"""

import os
import sys
import asyncio
import fitz  # PyMuPDF
from pathlib import Path
import hashlib
from datetime import datetime

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.config_service import ConfigService
from services.database_service import DatabaseService
from services.ai_service import AIService
from core.data_models import DocumentModel, DocumentType, ChunkModel, ChunkType

async def test_real_database_integration():
    """Test complete pipeline with real Supabase database"""
    print("Testing Real Database Integration with Supabase...")
    
    try:
        # Initialize services with real credentials
        print("Initializing services with real Supabase connection...")
        config_service = ConfigService()
        
        # Get Supabase credentials from environment
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
        
        if not supabase_url or not supabase_key:
            print("ERROR: SUPABASE_URL and SUPABASE_ANON_KEY environment variables not set!")
            print("Please set your Supabase credentials:")
            print("export SUPABASE_URL='https://your-project.supabase.co'")
            print("export SUPABASE_ANON_KEY='your-anon-key'")
            return False
        
        print(f"Supabase URL: {supabase_url}")
        print(f"Supabase Key: {supabase_key[:20]}...")
        
        # Initialize database service with real credentials
        database_service = DatabaseService(
            supabase_url=supabase_url,
            supabase_key=supabase_key
        )
        
        # Connect to database
        await database_service.connect()
        
        ai_service = AIService()
        
        # Test database connection
        print("Testing database connection...")
        try:
            connection_test = await database_service.test_connection()
            print(f"Database connection test: {connection_test}")
        except Exception as e:
            print(f"Database connection failed: {e}")
            return False
        
        # PDF file path
        pdf_path = r"C:\Users\haast\Downloads\HP_X580_SM.pdf"
        
        if not os.path.exists(pdf_path):
            print(f"ERROR: PDF file not found: {pdf_path}")
            return False
        
        print(f"PDF file found: {pdf_path}")
        print(f"File size: {os.path.getsize(pdf_path)} bytes")
        
        # Calculate file hash
        with open(pdf_path, 'rb') as f:
            file_content = f.read()
            file_hash = hashlib.sha256(file_content).hexdigest()
        
        print(f"File hash: {file_hash[:16]}...")
        
        # Extract text from PDF
        print("Extracting text from PDF...")
        doc = fitz.open(pdf_path)
        full_text = ""
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text = page.get_text()
            full_text += f"\n--- Page {page_num + 1} ---\n{text}"
        
        page_count = doc.page_count
        doc.close()
        
        print(f"Extracted text length: {len(full_text)} characters")
        print(f"Number of pages: {page_count}")
        
        # Create document in real database
        print("Creating document in real Supabase database...")
        
        document = DocumentModel(
            # id will be auto-generated by the model
            filename=os.path.basename(pdf_path),
            original_filename=os.path.basename(pdf_path),
            file_size=os.path.getsize(pdf_path),
            file_hash=file_hash,
            document_type=DocumentType.SERVICE_MANUAL,
            manufacturer="HP",
            series="Color LaserJet Enterprise",
            models=["X580"],
            version="1.0",
            language="en"
        )
        
        # Save document to real database
        try:
            created_doc = await database_service.create_document(document)
            if hasattr(created_doc, 'id'):
                print(f"Document created in real database: {created_doc.id}")
            else:
                print(f"Document created in real database: {created_doc}")
        except Exception as e:
            print(f"Failed to create document in database: {e}")
            return False
        
        # Test AI-powered chunking
        print("Testing AI-powered chunking...")
        
        # Get chunking strategy
        try:
            chunking_strategy = config_service.get_chunking_strategy()
            print(f"Chunking strategy: {chunking_strategy}")
        except:
            chunking_strategy = "contextual_chunking"
            print(f"Using default chunking strategy: {chunking_strategy}")
        
        # Create chunks with AI assistance
        print("Creating chunks with AI assistance...")
        chapters = full_text.split("Chapter ")
        chunks = []
        
        for i, chapter in enumerate(chapters):
            if chapter.strip() and len(chapter.strip()) > 100:
                chunk_data = {
                    'content': chapter.strip(),
                    'section_title': f"Chapter {i}" if i > 0 else "Introduction",
                    'confidence': 0.8,
                    'metadata': {
                        'chunk_type': 'chapter', 
                        'chunk_index': i, 
                        'source': 'hp_x580_sm',
                        'document_id': created_doc.id if hasattr(created_doc, 'id') else created_doc
                    }
                }
                chunks.append(chunk_data)
        
        print(f"Created {len(chunks)} chunks")
        
        # Save chunks to real database
        print("Saving chunks to real Supabase database...")
        saved_chunks = 0
        
        for i, chunk in enumerate(chunks):
            try:
                chunk_model = ChunkModel(
                    # id will be auto-generated by the model
                    document_id=created_doc.id if hasattr(created_doc, 'id') else created_doc,
                    content=chunk['content'],
                    chunk_type=ChunkType.TEXT,  # Required field
                    chunk_index=i,  # Required field
                    section_title=chunk['section_title'],
                    confidence_score=chunk['confidence'],
                    language="en"
                )
                
                # Save chunk to real database
                await database_service.create_chunk(chunk_model)
                saved_chunks += 1
                
                if saved_chunks % 50 == 0:
                    print(f"Saved {saved_chunks}/{len(chunks)} chunks to real database...")
                    
            except Exception as e:
                print(f"Error saving chunk {i}: {e}")
                continue
        
        print(f"Successfully saved {saved_chunks}/{len(chunks)} chunks to real database")
        
        # Test AI chunking with Ollama
        print("Testing AI chunking with Ollama...")
        try:
            # Test if Ollama is available
            ollama_models = ai_service.get_available_models()
            print(f"Available Ollama models: {ollama_models}")
            
            # Test text classification
            test_text = "This is a printer maintenance procedure for HP Color LaserJet Enterprise X580"
            classification = await ai_service.classify_text(test_text)
            print(f"AI Classification result: {classification}")
            
        except Exception as e:
            print(f"AI chunking test failed: {e}")
            print("Continuing without AI chunking...")
        
        # Verify data in database
        print("Verifying data in database...")
        try:
            # Try to retrieve the document
            retrieved_doc = await database_service.get_document(created_doc.id)
            print(f"Document retrieved from database: {retrieved_doc.filename}")
            
            # Try to retrieve some chunks
            chunks_count = await database_service.get_chunks_count(created_doc.id)
            print(f"Chunks count in database: {chunks_count}")
            
        except Exception as e:
            print(f"Failed to verify data in database: {e}")
        
        # Display final results
        print(f"\nFinal Results:")
        print(f"   Document ID: {created_doc.id if hasattr(created_doc, 'id') else created_doc}")
        print(f"   Total chunks: {len(chunks)}")
        print(f"   Saved chunks: {saved_chunks}")
        print(f"   Success rate: {saved_chunks/len(chunks)*100:.1f}%")
        
        # Test chunk size distribution
        chunk_sizes = [len(chunk['content']) for chunk in chunks]
        print(f"\nChunk Statistics:")
        print(f"   Min size: {min(chunk_sizes)} characters")
        print(f"   Max size: {max(chunk_sizes)} characters")
        print(f"   Average size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters")
        
        print("\nReal database integration test completed successfully!")
        return True
        
    except Exception as e:
        print(f"Real database integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_real_database_integration())
