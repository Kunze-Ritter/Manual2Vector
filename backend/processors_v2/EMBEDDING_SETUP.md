# ğŸ” Embedding Setup Guide

This guide helps you set up the embedding processor for semantic search.

---

## âš ï¸ Why Are Embeddings Important?

**Embeddings enable semantic search!**

Without embeddings:
- âŒ No similarity search
- âŒ No "find similar documents"
- âŒ No intelligent search results
- âŒ Only keyword-based search

With embeddings:
- âœ… Semantic similarity search
- âœ… "Find documents similar to this"
- âœ… Context-aware search results
- âœ… Better search experience

**â†’ Embeddings are REQUIRED for production use!**

---

## ğŸ“‹ Requirements

### 1. **Ollama** (Local AI Server)
- Runs the embedding model locally
- Fast and private
- No API costs

### 2. **embeddinggemma Model**
- 768-dimensional embeddings
- Optimized for semantic search
- Good balance of speed and quality

### 3. **Supabase with pgvector**
- Stores embeddings
- Vector similarity search
- Already configured in your database

---

## ğŸš€ Setup Instructions

### Step 1: Install Ollama

**Windows:**
```powershell
# Download from: https://ollama.ai/download
# Or use winget:
winget install Ollama.Ollama
```

**Linux/Mac:**
```bash
curl https://ollama.ai/install.sh | sh
```

### Step 2: Start Ollama Service

```bash
ollama serve
```

**Note:** On Windows, Ollama usually runs as a background service automatically.

### Step 3: Pull the Embedding Model

```bash
ollama pull embeddinggemma
```

This downloads the `embeddinggemma` model (~274MB).

### Step 4: Verify Configuration

```bash
cd backend/processors_v2
python test_embedding_config.py
```

**Expected output (with full config):**
```
ğŸ“„ Loading .env from: C:\Users\...\KRAI-minimal\.env

ğŸ”‘ Checking Supabase credentials...
  â€¢ SUPABASE_URL: âœ“ Set
  â€¢ SUPABASE_SERVICE_ROLE_KEY: âœ“ Set
  â€¢ SUPABASE_ANON_KEY: âœ“ Set
  â€¢ Using Key: âœ“ Available
âœ… Supabase client created successfully
âœ… Supabase connection verified (can query database)

ğŸ“Š Configuration Status:
  âœ“ Is Configured: True
  âœ“ Ollama Available: True
  âœ“ Ollama URL: http://localhost:11434
  âœ“ Model Name: embeddinggemma
  âœ“ Embedding Dimension: 768
  âœ“ Batch Size: 100
  âœ“ Supabase Configured: True

âœ… EMBEDDING PROCESSOR IS FULLY CONFIGURED!
   Ready to generate embeddings for semantic search.

ğŸ§ª Testing embedding generation...
âœ… Test embedding generated successfully!
   â€¢ Dimension: 768
   â€¢ Sample values: [0.123, -0.456, 0.789]
```

**Note:** The test script loads credentials from your `.env` file. In the actual processing pipeline, Supabase is automatically configured by the MasterPipeline.

---

## ğŸ”§ Troubleshooting

### Problem: "Ollama is not available"

**Solution:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not running, start it:
ollama serve

# Check if model is installed:
ollama list

# If embeddinggemma not in list:
ollama pull embeddinggemma
```

### Problem: "Model embeddinggemma not found"

**Solution:**
```bash
# Pull the model:
ollama pull embeddinggemma

# Wait for download to complete, then verify:
ollama list
```

### Problem: "Supabase client not configured"

**Solution:**
- Check your `.env` file
- Verify `SUPABASE_URL` and `SUPABASE_KEY` are set
- Make sure supabase_client is passed to DocumentProcessor

### Problem: "Connection refused to localhost:11434"

**Solutions:**
1. Ollama not running â†’ `ollama serve`
2. Different port â†’ Set `OLLAMA_URL` in .env
3. Firewall blocking â†’ Allow localhost connections

---

## ğŸ§ª Test Embedding Generation

### Quick Test

```python
from processors_v2.embedding_processor import EmbeddingProcessor

processor = EmbeddingProcessor()

if processor.is_configured():
    # Test embedding
    text = "This is a test sentence"
    embedding = processor._generate_embedding(text)
    print(f"âœ… Generated {len(embedding)}-dimensional embedding")
else:
    print("âŒ Not configured")
    status = processor.get_configuration_status()
    print(status)
```

### Full Test with Script

```bash
python backend/processors_v2/test_embedding_config.py
```

---

## ğŸ“Š Configuration Check

The embedding processor checks configuration on startup:

```python
# In your processing pipeline:
emb_status = processor.get_configuration_status()

if emb_status['is_configured']:
    print("âœ… Ready to generate embeddings")
else:
    print("âŒ Configuration problem:")
    print(f"  â€¢ Ollama: {emb_status['ollama_available']}")
    print(f"  â€¢ Supabase: {emb_status['supabase_configured']}")
```

---

## âš™ï¸ Configuration Options

### Environment Variables

```bash
# .env file
OLLAMA_URL=http://localhost:11434  # Ollama API URL
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=your-key-here
```

### Code Configuration

```python
from processors_v2.embedding_processor import EmbeddingProcessor

processor = EmbeddingProcessor(
    supabase_client=supabase,
    ollama_url="http://localhost:11434",
    model_name="embeddinggemma",
    batch_size=100,
    embedding_dimension=768
)
```

---

## ğŸ¯ Expected Performance

With proper configuration:
- **Speed:** ~100-200 chunks/second
- **Dimension:** 768D vectors
- **Storage:** ~3KB per chunk (768 floats * 4 bytes)
- **Quality:** Excellent for semantic search

Example output:
```
INFO     Generating embeddings for 96 chunks...
INFO     Processing batch 1/1 (96 chunks)...
SUCCESS  Created 96 embeddings in 0.5s (192.0 chunks/s)
```

---

## ğŸ“ Integration Checklist

Before processing documents:

- [ ] Ollama is installed
- [ ] Ollama service is running (`ollama serve`)
- [ ] embeddinggemma model is installed (`ollama pull embeddinggemma`)
- [ ] Supabase credentials are in .env
- [ ] Test script passes (`python test_embedding_config.py`)
- [ ] Pipeline shows "âœ… Embedding processor configured"

---

## ğŸš¨ Production Checklist

For production use:

- [ ] Ollama runs as a system service (auto-start)
- [ ] Sufficient RAM (4GB+ recommended for embeddings)
- [ ] Supabase pgvector extension enabled
- [ ] Monitoring for Ollama uptime
- [ ] Fallback handling if embeddings fail

---

## ğŸ“š Related Documentation

- [Ollama Documentation](https://github.com/ollama/ollama)
- [embeddinggemma Model](https://ollama.ai/library/embeddinggemma)
- [Supabase pgvector Guide](https://supabase.com/docs/guides/ai/vector-columns)

---

## â“ FAQ

**Q: Can I use a different embedding model?**
A: Yes, change `model_name` parameter. Models: `embeddinggemma`, `nomic-embed-text`, `mxbai-embed-large`

**Q: What if Ollama is down during processing?**
A: Embeddings will be skipped with a warning. Documents are still processed, but semantic search won't work.

**Q: How much disk space do embeddings need?**
A: ~3KB per chunk. For 10,000 chunks = ~30MB.

**Q: Can I run Ollama on a different machine?**
A: Yes, set `OLLAMA_URL=http://other-machine:11434` in .env

**Q: Do I need a GPU?**
A: No, CPU is fine for embeddings. GPU speeds it up but isn't required.

---

## âœ… Success Criteria

You'll know it's working when you see:

```
ğŸ” CHECKING EMBEDDING CONFIGURATION...
âœ… Embedding processor configured and ready
   â€¢ Ollama: http://localhost:11434
   â€¢ Model: embeddinggemma (768D)

INFO     Generating embeddings for 96 chunks...
SUCCESS  Created 96 embeddings in 0.8s (120.0 chunks/s)

ğŸ“¦ Extracted:
   ...
   Embeddings: 96  â† This should appear!
```

**If you see this â†’ Everything is working! ğŸ‰**
