    # ===========================================
# DATABASE ADAPTER CONFIGURATION
# ===========================================
# Database type: 'postgresql', 'supabase', or 'mysql'
# Default: 'postgresql' (local Docker)
DATABASE_TYPE=postgresql

# ===========================================
# GENERIC DATABASE CONFIGURATION
# ===========================================
# Required for DATABASE_TYPE=postgresql
# Alternative to individual DATABASE_* parameters
DATABASE_URL=postgresql://krai_user:krai_secure_password@localhost:5432/krai

# Individual connection parameters (alternative to URL)
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=krai
DATABASE_USER=krai_user
DATABASE_PASSWORD=krai_secure_password

# Schema prefix for multi-tenant support (default: 'krai')
# Creates schemas: krai_core, krai_content, krai_intelligence, krai_system, krai_parts
DATABASE_SCHEMA_PREFIX=krai

# ===========================================
# GENERIC OBJECT STORAGE CONFIGURATION
# ===========================================
# Storage type: 's3' (works with MinIO, R2, AWS S3, etc.)
OBJECT_STORAGE_TYPE=s3

# S3-compatible endpoint (MinIO local, R2 cloud, AWS S3, etc.)
OBJECT_STORAGE_ENDPOINT=http://localhost:9000

# S3-compatible credentials
OBJECT_STORAGE_ACCESS_KEY=minioadmin
OBJECT_STORAGE_SECRET_KEY=minioadmin123

# S3 region (us-east-1 for MinIO, auto for R2, etc.)
OBJECT_STORAGE_REGION=us-east-1

# SSL configuration
OBJECT_STORAGE_USE_SSL=false

# Generic bucket names (vendor-agnostic)
OBJECT_STORAGE_BUCKET_DOCUMENTS=documents
OBJECT_STORAGE_BUCKET_IMAGES=images
OBJECT_STORAGE_BUCKET_VIDEOS=videos
OBJECT_STORAGE_BUCKET_TEMP=temp

# Public URL for object storage access (MinIO Console, R2 public URLs, etc.)
OBJECT_STORAGE_PUBLIC_URL=http://localhost:9000

# ===========================================
# GENERIC AI SERVICE CONFIGURATION
# ===========================================
# AI service type: 'ollama', 'openai', 'anthropic'
AI_SERVICE_TYPE=ollama

# AI service endpoint
AI_SERVICE_URL=http://localhost:11434

# AI models
AI_EMBEDDING_MODEL=nomic-embed-text:latest
AI_EMBEDDING_DIMENSION=768
AI_TEXT_MODEL=llama3.2:latest
AI_VISION_MODEL=llava-phi3:latest

# ===========================================
# MULTI-MODAL EMBEDDING CONFIGURATION
# ===========================================
# Visual embeddings (ColQwen2.5)
AI_VISUAL_EMBEDDING_MODEL=vidore/colqwen2.5-v0.2
AI_VISUAL_EMBEDDING_DIMENSION=768
ENABLE_VISUAL_EMBEDDINGS=true

# Table extraction settings
ENABLE_TABLE_EXTRACTION=true
TABLE_EXTRACTION_STRATEGY=lines

# Multi-modal embeddings v2 table (unified storage)
ENABLE_EMBEDDINGS_V2=false
    
    # ===========================================
# AUTHENTICATION DEFAULTS
# ===========================================
# Default admin account created during startup/bootstrap
DEFAULT_ADMIN_EMAIL=admin@example.com
DEFAULT_ADMIN_USERNAME=admin
DEFAULT_ADMIN_FIRST_NAME=System
DEFAULT_ADMIN_LAST_NAME=Administrator
# Leave blank to require manual password entry (startup/event/script)
DEFAULT_ADMIN_PASSWORD=

# ===========================================
# PROCESSING PIPELINE SETTINGS
# ===========================================
# Control which extraction steps are enabled
# Set to 'false' to skip specific extraction steps

# Extract text content from documents
ENABLE_TEXT_EXTRACTION=true

# Extract images from PDFs and documents
ENABLE_IMAGE_EXTRACTION=true

# Extract video content and metadata
ENABLE_VIDEO_EXTRACTION=true

# Extract tables from documents
ENABLE_TABLE_EXTRACTION=true

# Extract SVG graphics and diagrams
ENABLE_SVG_EXTRACTION=true

# Extract links and embedded content
ENABLE_LINK_EXTRACTION=true

# Generate text embeddings for semantic search
ENABLE_TEXT_EMBEDDINGS=true

# Generate visual embeddings for image similarity
ENABLE_VISUAL_EMBEDDINGS=true

# Generate table embeddings for structured data search
ENABLE_TABLE_EMBEDDINGS=true

# Extract contextual information around entities
ENABLE_CONTEXT_EXTRACTION=true

# Context window size (characters before/after entities)
CONTEXT_WINDOW_SIZE=200

# Extract product models (recommended: true)
ENABLE_PRODUCT_EXTRACTION=true

# Extract spare parts (recommended: true for service manuals)
ENABLE_PARTS_EXTRACTION=true

# Extract error codes (recommended: true for service manuals)
ENABLE_ERROR_CODE_EXTRACTION=true

# Extract document versions (recommended: true)
ENABLE_VERSION_EXTRACTION=true

# Run OCR on images (recommended: true)
ENABLE_OCR=true

# Run Vision AI on images (optional: false if low VRAM)
ENABLE_VISION_AI=true

# Generate embeddings for semantic search (recommended: true)
ENABLE_EMBEDDINGS=true

# ===========================================
# PRODUCT RESEARCH CONFIGURATION
# ===========================================
# Enable automatic online research for unknown products
ENABLE_PRODUCT_RESEARCH=true

# Tavily API for web search (optional, recommended)
# Get free API key at: https://tavily.com
# Without this, system will use direct manufacturer website URLs
TAVILY_API_KEY=your_tavily_api_key_here

# Research cache duration (days)
RESEARCH_CACHE_DAYS=90

# LLM Product Extraction - Maximum pages to scan
# Default: 20 (scans first 20 pages for product info)
# Set to 0 to scan ALL pages (slower but more thorough)
# Set to 50 for medium coverage
# Why limit? LLM is slow (~8s/page). 278 pages = 37 minutes!
# Product specs are usually in first 20 pages anyway.
LLM_MAX_PAGES=20

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5

# ===========================================
# FIRECRAWL WEB SCRAPING CONFIGURATION
# ===========================================
# Firecrawl is optional. Keep 'beautifulsoup' for legacy HTML scraping.
# For Firecrawl, run krai-redis, krai-playwright, krai-firecrawl-api, krai-firecrawl-worker via Docker Compose.
# Ollama integration is automatic; OpenAI can be enabled for premium extraction quality.

# Web scraping backend. 'firecrawl' enables JS rendering + LLM extraction; 'beautifulsoup' keeps basic HTML parsing.
SCRAPING_BACKEND=beautifulsoup

# Firecrawl self-hosted API endpoint. Use http://localhost:3002 externally, http://krai-firecrawl-api:3002 in Docker.
FIRECRAWL_API_URL=http://localhost:3002
FIRECRAWL_BULL_AUTH_KEY=changeme_firecrawl_admin

# LLM provider for Firecrawl's structured extraction (ollama=open source, openai=cloud).
FIRECRAWL_LLM_PROVIDER=ollama
FIRECRAWL_MODEL_NAME=llama3.2:latest
FIRECRAWL_EMBEDDING_MODEL=nomic-embed-text:latest

# Optional: add OpenAI API key when FIRECRAWL_LLM_PROVIDER=openai (https://platform.openai.com/api-keys).
OPENAI_API_KEY=

# Advanced Firecrawl settings. Tune concurrency to available RAM (4 ≈ 4GB). Blocking media speeds up scraping.
FIRECRAWL_ALLOW_LOCAL_WEBHOOKS=true
FIRECRAWL_MAX_CONCURRENCY=4
FIRECRAWL_BLOCK_MEDIA=true

# Optional proxy configuration for outbound scraping requests.
FIRECRAWL_PROXY_SERVER=
FIRECRAWL_PROXY_USERNAME=
FIRECRAWL_PROXY_PASSWORD=

# Experimental Firecrawl features. Link enrichment scrapes URLs inside PDFs. Manufacturer crawling polls vendor sites.
ENABLE_LINK_ENRICHMENT=false
ENABLE_MANUFACTURER_CRAWLING=false

# ===========================================
# OBJECT STORAGE UPLOAD SETTINGS
# ===========================================
# Control what gets uploaded to object storage
# Images are always saved to database, these settings control cloud upload

# Upload extracted images to object storage (recommended: true)
# Images will have public URLs for viewing
UPLOAD_IMAGES_TO_STORAGE=false

# Upload original PDF documents to object storage (optional: false)
# PDFs stay local by default, only enable if you need cloud backup
UPLOAD_DOCUMENTS_TO_STORAGE=false

# ===========================================
# LOGGING CONFIGURATION
# ===========================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: most verbose diagnostics (development recommended)
# INFO: balanced operational visibility (production default)
# WARNING: only potential issues and failures
# ERROR: failures requiring attention
# CRITICAL: only catastrophic issues
LOG_LEVEL=INFO

# Enable console logging
LOG_TO_CONSOLE=true

# Enable file logging
LOG_TO_FILE=true

# Log directory (default: backend/logs)
LOG_DIR=backend/logs

# Log rotation mode: size (file size) or time (time-based)
LOG_ROTATION=size

# Maximum log file size in bytes (for size-based rotation)
# Default: 10MB
LOG_MAX_BYTES=10000000

# Number of backup log files to keep
# Example rotation: processor_v2.log → processor_v2.log.1, processor_v2.log.2 ...
# Files beyond the configured count are removed automatically
LOG_BACKUP_COUNT=5

# Time-based rotation settings (only used if LOG_ROTATION=time)
# When to rotate: midnight, W0-W6 (weekday), or H (hourly)
LOG_ROTATION_WHEN=midnight

# Rotation interval (e.g., 1 for daily with midnight)
LOG_ROTATION_INTERVAL=1

# ===========================================
# OCR FALLBACK CONFIGURATION
# ===========================================
# Enable OCR fallback for pages without extractable text
# Requires: pip install pytesseract pillow
# Also requires Tesseract binary: https://github.com/tesseract-ocr/tesseract
ENABLE_OCR_FALLBACK=false

# ===========================================
# EXTERNAL API KEYS
# ===========================================
# YouTube Data API v3
# Get your key at: https://console.cloud.google.com/apis/credentials
# Free quota: 10,000 units/day (1 video = 1 unit)
YOUTUBE_API_KEY=your_youtube_api_key_here

# ===========================================
# LEGACY CLOUD CONFIGURATION (FOR BACKWARD COMPATIBILITY)
# ===========================================
# These variables are kept for backward compatibility
# Use DATABASE_TYPE=supabase and OBJECT_STORAGE_TYPE=s3 with R2 endpoint to enable

# Supabase cloud configuration (use DATABASE_TYPE=supabase)
# SUPABASE_URL=https://crujfdpqdjzcfqeyhang.supabase.co
# SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
# SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
# SUPABASE_STORAGE_URL=https://crujfdpqdjzcfqeyhang.supabase.co/storage/v1

# Cloudflare R2 configuration (use OBJECT_STORAGE_TYPE=s3 with R2 endpoint)
# R2_ACCESS_KEY_ID=9c59473961632448c91db3ef9dbd35ab
# R2_SECRET_ACCESS_KEY=9cc62a9506ac9ec6e8373a39fa86268bc187632e...
# R2_BUCKET_NAME_DOCUMENTS=krai-documents-images
# R2_BUCKET_NAME_ERROR=krai-error-images
# R2_BUCKET_NAME_PARTS=krai-parts-images
# R2_ENDPOINT_URL=https://a88f92c913c232559845adb9001a5d14.eu.r2.cloudflarestorage.com
# R2_REGION=auto

# Legacy Ollama configuration (alias for AI_SERVICE_URL)
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest
# OLLAMA_MODEL_TEXT=llama3.2:latest
# OLLAMA_MODEL_VISION=bakllava:7b
# DISABLE_VISION_PROCESSING=false

# ===========================================
# PHASE 6: HIERARCHICAL CHUNKING & MULTIMODAL SEARCH
# ===========================================

# Hierarchical Chunking Configuration
ENABLE_HIERARCHICAL_CHUNKING=false
DETECT_ERROR_CODE_SECTIONS=true
LINK_CHUNKS=true
HIERARCHICAL_CHUNK_MIN_SIZE=200
HIERARCHICAL_CHUNK_MAX_SIZE=1000
HIERARCHICAL_CHUNK_OVERLAP=50

# Multimodal Search Configuration
ENABLE_MULTIMODAL_SEARCH=true
MULTIMODAL_SEARCH_THRESHOLD=0.5
MULTIMODAL_SEARCH_LIMIT=10
ENABLE_TWO_STAGE_SEARCH=true
TWO_STAGE_FIRST_LIMIT=50
TWO_STAGE_FINAL_LIMIT=10
CONTEXT_BOOST_FACTOR=0.2

# SVG Extraction Configuration
ENABLE_SVG_EXTRACTION=false
SVG_CONVERSION_DPI=300
SVG_MAX_DIMENSION=2048
SVG_SCALE_IMAGES=true

# Advanced Search Features
ENABLE_CONTEXT_AWARE_SEARCH=true
ENABLE_RESULT_ENRICHMENT=true
ENABLE_SEMANTIC_RERANKING=true
SEARCH_TIMEOUT_SECONDS=30