# Vision Model Alternativen zu LLaVA

## üéØ Aktuell verwendet: LLaVA

**Problem**: Crasht manchmal trotz aller Optimierungen

---

## üîÑ Ollama Vision Model Alternativen

### **1. BakLLaVA** (Empfohlen als Alternative)
**Modell**: `bakllava:7b`  
**VRAM**: ~4-5 GB  
**Vorteile**:
- ‚úÖ Basiert auf Mistral (statt Llama)
- ‚úÖ Oft stabiler als LLaVA
- ‚úÖ Gleiche Gr√∂√üe wie llava:7b
- ‚úÖ Gute Performance bei technischen Bildern

**Installation**:
```bash
ollama pull bakllava:7b
```

**Verwendung in .env**:
```bash
OLLAMA_MODEL_VISION=bakllava:7b
```

---

### **2. LLaVA-Phi** (Kleiner & Schneller)
**Modell**: `llava-phi3:latest`  
**VRAM**: ~2-3 GB  
**Vorteile**:
- ‚úÖ Viel kleiner (3.8B Parameter)
- ‚úÖ Schneller
- ‚úÖ Weniger VRAM
- ‚úÖ Basiert auf Phi-3

**Nachteile**:
- ‚ö†Ô∏è Weniger genau als 7B Modelle
- ‚ö†Ô∏è Schlechter bei komplexen technischen Bildern

**Installation**:
```bash
ollama pull llava-phi3
```

**Verwendung**:
```bash
OLLAMA_MODEL_VISION=llava-phi3:latest
```

---

### **3. Moondream** (Sehr klein)
**Modell**: `moondream:latest`  
**VRAM**: ~1-2 GB  
**Vorteile**:
- ‚úÖ Sehr klein (1.6B Parameter)
- ‚úÖ Extrem schnell
- ‚úÖ L√§uft auf fast jeder GPU
- ‚úÖ Sehr stabil

**Nachteile**:
- ‚ö†Ô∏è Deutlich weniger genau
- ‚ö†Ô∏è Nur f√ºr einfache Bildanalyse
- ‚ö†Ô∏è Nicht gut f√ºr Error Code Extraction

**Installation**:
```bash
ollama pull moondream
```

**Verwendung**:
```bash
OLLAMA_MODEL_VISION=moondream:latest
```

---

### **4. MiniCPM-V** (Neu & Effizient)
**Modell**: `minicpm-v:latest`  
**VRAM**: ~4 GB  
**Vorteile**:
- ‚úÖ Effizientes 2.8B Modell
- ‚úÖ Gute Performance/Size Ratio
- ‚úÖ Optimiert f√ºr Effizienz
- ‚úÖ Unterst√ºtzt mehrere Sprachen

**Nachteile**:
- ‚ö†Ô∏è Relativ neu
- ‚ö†Ô∏è Weniger getestet

**Installation**:
```bash
ollama pull minicpm-v
```

**Verwendung**:
```bash
OLLAMA_MODEL_VISION=minicpm-v:latest
```

---

## üìä Vergleich

| Modell | VRAM | Parameter | Geschwindigkeit | Genauigkeit | Stabilit√§t | Empfehlung |
|--------|------|-----------|-----------------|-------------|------------|------------|
| **llava:7b** | 4-5 GB | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Aktuell |
| **bakllava:7b** | 4-5 GB | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Beste Alternative |
| **llava-phi3** | 2-3 GB | 3.8B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | F√ºr schwache GPUs |
| **minicpm-v** | 4 GB | 2.8B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Interessant |
| **moondream** | 1-2 GB | 1.6B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Nur Notfall |

---

## üöÄ Empfohlene Reihenfolge zum Testen

### **F√ºr 8GB VRAM (RTX 2000):**
1. ‚úÖ **bakllava:7b** (Erste Alternative)
2. ‚úÖ **minicpm-v** (Wenn bakllava auch crasht)
3. ‚úÖ **llava-phi3** (Wenn alles andere crasht)
4. ‚ùå **moondream** (Nur als letzte Option)

### **F√ºr 16GB+ VRAM:**
1. ‚úÖ **bakllava:7b** (Sollte stabiler sein)
2. ‚úÖ **llava:13b** / **llava:latest** (Original in gr√∂√üer)

---

## üîß Automatisches Fallback konfigurieren

Aktuell im Code (ai_service.py):
```python
# Vision fallbacks - DISABLED due to VRAM issues
'llava:latest': ['llava:7b'],
'llava:7b': [],  # No fallbacks
'bakllava:latest': [],
```

**Empfohlen: Fallback aktivieren**

```python
# Vision fallbacks
'llava:7b': ['bakllava:7b', 'llava-phi3:latest'],
'llava:latest': ['llava:7b', 'bakllava:7b'],
'bakllava:7b': ['llava-phi3:latest', 'moondream:latest'],
```

**ABER**: Fallback bedeutet bei Crash wird anderes Model probiert ‚Üí Kann noch mehr Probleme verursachen

---

## üß™ Schnelltest f√ºr Alternativen

```bash
# BakLLaVA testen
ollama pull bakllava:7b
ollama run bakllava:7b

# Danach in .env:
OLLAMA_MODEL_VISION=bakllava:7b

# Pipeline laufen lassen
python backend/tests/krai_master_pipeline.py
```

---

## üí° Warum BakLLaVA empfohlen?

**Basiert auf Mistral statt Llama:**
- ‚úÖ Andere Architektur ‚Üí Andere Speicherverwaltung
- ‚úÖ Oft stabiler bei wiederholten Aufrufen
- ‚úÖ Gute Performance bei technischen Dokumenten
- ‚úÖ Gleiche VRAM-Anforderungen wie llava:7b

**Community-Feedback:**
- Weniger "model runner stopped" Fehler
- Besser f√ºr Batch-Verarbeitung
- Stabiler bei langen Sessions

---

## üéØ Alternative: Vision komplett deaktivieren

**Option 1**: Nur Pattern-Matching (kein Vision AI)
```bash
DISABLE_VISION_PROCESSING=true
```
- ‚úÖ 100% stabil
- ‚úÖ Kein VRAM-Problem
- ‚ùå Keine Error Codes aus Screenshots

**Option 2**: Externes Vision API nutzen
- OpenAI GPT-4 Vision (kostenpflichtig)
- Google Gemini Vision (kostenpflichtig)
- Anthropic Claude Vision (kostenpflichtig)

---

## üìù Zusammenfassung

### **Wenn llava:7b crasht:**

**Schnelle L√∂sung:**
```bash
# 1. BakLLaVA installieren
ollama pull bakllava:7b

# 2. In .env √§ndern
OLLAMA_MODEL_VISION=bakllava:7b

# 3. Pipeline neu starten
```

**Wenn auch BakLLaVA crasht:**
```bash
# Kleineres Modell
ollama pull llava-phi3
OLLAMA_MODEL_VISION=llava-phi3:latest
```

**Wenn alles crasht:**
```bash
# Vision AI deaktivieren
DISABLE_VISION_PROCESSING=true
```

---

## üîç Weitere Optionen (Zukunft)

Ollama entwickelt st√§ndig neue Vision Models:
- LLaVA 1.6 (verbessert)
- Obsidian (neu)
- CogVLM (sehr gro√ü, aber sehr gut)

Check: `ollama list` und `ollama search vision`

---

**Empfehlung**: **BakLLaVA:7b als erste Alternative testen!** üöÄ

**Datum**: Oktober 2, 2025
