# ===========================================
# AI SERVICE CONFIGURATION (Ollama)
# ===========================================

# Ollama Server URL
OLLAMA_URL=http://localhost:11434

# AI Models
OLLAMA_MODEL_EMBEDDING=embeddinggemma:latest
OLLAMA_MODEL_TEXT=llama3.2:latest

# Vision Model - Auto-detected based on GPU VRAM
# Override here if you want a specific model
# bakllava:7b = 4GB VRAM (recommended - more stable than llava)
# llava:7b = 4GB VRAM (alternative)
# llava:latest = 11GB VRAM (for 12GB+ GPU)
OLLAMA_MODEL_VISION=llava:7b

# Vision Processing Settings
# Set to 'true' to skip AI image analysis (faster, uses less VRAM)
# The pipeline will still extract images but won't analyze them with AI
DISABLE_VISION_PROCESSING=false

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5
