# ===========================================
# AI SERVICE CONFIGURATION (Ollama)
# ===========================================

# Ollama Server URL
OLLAMA_URL=http://localhost:11434

# AI Models
OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest
OLLAMA_MODEL_TEXT=llama3.2:latest

# Vision Model - Auto-detected based on GPU VRAM
# Override here if you want a specific model
# bakllava:7b = 4GB VRAM (recommended - more stable than llava)
# llava:7b = 4GB VRAM (alternative)
# llava:latest = 11GB VRAM (for 12GB+ GPU)
OLLAMA_MODEL_VISION=llava:7b

# Vision Processing Settings
# Set to 'true' to skip AI image analysis (faster, uses less VRAM)
# The pipeline will still extract images but won't analyze them with AI
DISABLE_VISION_PROCESSING=false

# LLM Product Extraction - Maximum pages to scan
# Default: 20 (scans first 20 pages for product info)
# Set to 0 to scan ALL pages (VERY SLOW! 694 pages = 92 minutes!)
# Set to 50 for medium coverage (recommended - 7 minutes)
# Why limit? LLM is slow (~8s/page). 278 pages = 37 minutes!
# Product specs are usually in first 20-50 pages anyway.
LLM_MAX_PAGES=30

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5
