# ===========================================
# AI SERVICE CONFIGURATION (Ollama)
# ===========================================

# Ollama Server URL
# Use host.docker.internal if Ollama runs in Docker
# Use localhost:11434 if Ollama runs locally
OLLAMA_URL=http://localhost:11434

# AI Models
OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest
OLLAMA_MODEL_TEXT=llama3.2:3b

# Vision Model - Auto-detected based on GPU VRAM
# Override here if you want a specific model
# bakllava:7b = 4GB VRAM (recommended - more stable than llava)
# llava:7b = 4GB VRAM (alternative)
# llava:latest = 11GB VRAM (for 12GB+ GPU)
OLLAMA_MODEL_VISION=llava:7b

# Vision Processing Settings
# Set to 'true' to skip AI image analysis (faster, uses less VRAM)
# The pipeline will still extract images but won't analyze them with AI
DISABLE_VISION_PROCESSING=false

# LLM Product Extraction - Maximum pages to scan
# Default: 20 (scans first 20 pages for product info)
# Set to 0 to scan ALL pages (VERY SLOW! 694 pages = 92 minutes!)
# Set to 50 for medium coverage (recommended - 7 minutes)
# Why limit? LLM is slow (~8s/page). 278 pages = 37 minutes!
# Product specs are usually in first 20-50 pages anyway.
LLM_MAX_PAGES=30

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5

# ===========================================
# GPU/CPU CONFIGURATION
# ===========================================
# Enable GPU acceleration for OpenCV and ML models
# Set to 'true' to use GPU (requires CUDA and opencv-contrib-python)
# Set to 'false' to use CPU only (default, works everywhere)
USE_GPU=false

# CUDA device to use (if USE_GPU=true)
# 0 = first GPU, 1 = second GPU, etc.
CUDA_VISIBLE_DEVICES=0
