"""
StageTracker Integration Tests

This module provides comprehensive integration testing for StageTracker functionality
with the pipeline processors. Tests cover stage lifecycle management, progress tracking,
error handling, and database interactions.

Test Categories:
1. Stage Lifecycle Tests
2. Progress Tracking Tests  
3. Error Handling Tests
4. Database Integration Tests
5. Context Management Tests
6. Performance Tests

All tests use the fixtures from conftest.py for consistent mock objects and test data.
"""

import pytest
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List
from unittest.mock import AsyncMock, MagicMock, patch

from backend.processors.stage_tracker import StageTracker
from backend.processors.upload_processor import UploadProcessor
from backend.processors.document_processor import DocumentProcessor
from backend.processors.text_processor_optimized import OptimizedTextProcessor
from backend.core.base_processor import ProcessingContext


pytestmark = pytest.mark.processor


class TestStageLifecycle:
    """Test stage lifecycle management."""
    
    @pytest.mark.asyncio
    async def test_complete_stage_lifecycle(self, mock_database_adapter, sample_pdf_files, processor_test_config, mock_stage_tracker):
        """Test complete stage lifecycle from start to completion."""
        # Arrange
        processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        valid_pdf = sample_pdf_files['valid_pdf']
        
        # Act
        result = await processor.process(valid_pdf['path'])
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # StageTracker should have been called for stage lifecycle
        # In real implementation, verify specific RPC calls were made
        # For now, verify processing succeeded with stage tracking enabled
    
    @pytest.mark.asyncio
    async def test_stage_start_and_completion(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test stage start and completion tracking."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        test_content = "Test content for stage lifecycle testing."
        test_file = temp_test_pdf / "stage_lifecycle_test.pdf"
        test_file.write_text(test_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        assert result.success, "Document processing should succeed"
        
        # Stage should have been started and completed
        # In real implementation, verify stage_tracker.start_stage and stage_tracker.complete_stage were called
    
    @pytest.mark.asyncio
    async def test_stage_failure_tracking(self, mock_database_adapter, sample_pdf_files, processor_test_config, mock_stage_tracker):
        """Test stage failure tracking."""
        # Arrange
        processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        # Use corrupted PDF to trigger failure
        corrupted_pdf = sample_pdf_files['corrupted_pdf']
        
        # Act
        result = await processor.process(corrupted_pdf['path'])
        
        # Assert
        assert not result.success, "Corrupted PDF should fail"
        
        # Stage failure should be tracked
        # In real implementation, verify stage_tracker.fail_stage was called
    
    @pytest.mark.asyncio
    async def test_stage_skip_tracking(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test stage skip tracking."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create empty PDF that might be skipped
        empty_file = temp_test_pdf / "empty_for_skip.pdf"
        empty_file.write_bytes(b"")
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=empty_file,
            metadata={'filename': empty_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        # Might fail or be skipped depending on implementation
        # In real implementation, verify stage_tracker.skip_stage was called if applicable
    
    @pytest.mark.asyncio
    async def test_multiple_stages_sequence(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test tracking of multiple stages in sequence."""
        # Arrange
        upload_processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        document_processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        text_processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            chunk_size=processor_test_config['chunk_size'],
            chunk_overlap=processor_test_config['chunk_overlap'],
            stage_tracker=mock_stage_tracker
        )
        
        test_content = """Multi-stage test content.
This content will be processed through multiple stages.
Each stage should be tracked independently.

Technical Specifications:
- Device: Test Device X
- Speed: 50 ppm
- Resolution: 1200 dpi

Error Codes:
- 900.01: Test error
- 900.02: Another test error"""
        
        test_file = temp_test_pdf / "multi_stage_test.pdf"
        test_file.write_text(test_content)
        
        # Act - Process through all stages
        # Stage 1: Upload
        upload_result = await upload_processor.process(test_file)
        assert upload_result.success, "Upload stage should succeed"
        
        document_id = upload_result.data['document_id']
        
        # Stage 2: Document Processing
        document_context = ProcessingContext(
            document_id=document_id,
            file_path=test_file,
            metadata=upload_result.metadata
        )
        
        document_result = await document_processor.process(document_context)
        assert document_result.success, "Document stage should succeed"
        
        # Stage 3: Text Processing
        text_context = ProcessingContext(
            document_id=document_id,
            file_path=test_file,
            metadata=document_result.data['metadata']
        )
        text_context.page_texts = document_result.data['page_texts']
        
        text_result = await text_processor.process(text_context)
        assert text_result.success, "Text stage should succeed"
        
        # Assert
        # All three stages should have been tracked
        # In real implementation, verify stage_tracker calls for each stage
        assert document_id is not None, "Should have document ID"
        assert len(text_result.data['chunks']) > 0, "Should have processed chunks"
    
    @pytest.mark.asyncio
    async def test_stage_context_manager(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test StageContext context manager."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        # Create test context
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "context_test.pdf",
            metadata={'filename': 'context_test.pdf'}
        )
        
        # Act & Assert - Test context manager
        async with stage_tracker.start_stage(context, "test_stage") as stage_context:
            assert stage_context is not None, "Stage context should be created"
            assert stage_context.document_id == context.document_id, "Stage context should preserve document ID"
            assert stage_context.stage_name == "test_stage", "Stage context should have stage name"
            
            # Simulate some work
            await asyncio.sleep(0.01)
            
            # Update progress
            await stage_tracker.update_progress(stage_context, 50, "Processing test content")
        
        # Context should be automatically completed
        # In real implementation, verify completion was called
    
    @pytest.mark.asyncio
    async def test_stage_context_with_error(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test StageContext with error handling."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "error_context_test.pdf",
            metadata={'filename': 'error_context_test.pdf'}
        )
        
        # Act & Assert - Test error in context manager
        try:
            async with stage_tracker.start_stage(context, "error_test_stage") as stage_context:
                # Simulate some work
                await asyncio.sleep(0.01)
                
                # Raise an error
                raise ValueError("Test error in stage processing")
        except ValueError:
            # Error should be caught and stage should be marked as failed
            pass
        
        # In real implementation, verify fail_stage was called


class TestProgressTracking:
    """Test progress tracking functionality."""
    
    @pytest.mark.asyncio
    async def test_progress_updates_during_processing(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress updates during document processing."""
        # Arrange
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=50,  # Small chunks to trigger more progress updates
            chunk_overlap=10,
            stage_tracker=mock_stage_tracker
        )
        
        # Create larger content for progress tracking
        large_content = " ".join([f"Progress test sentence {i}." for i in range(100)])
        
        test_file = temp_test_pdf / "progress_test.pdf"
        test_file.write_text(large_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # Progress updates should have been called during chunking
        # In real implementation, verify update_progress calls
        chunks = result.data['chunks']
        assert len(chunks) > 1, "Should create multiple chunks for progress testing"
    
    @pytest.mark.asyncio
    async def test_progress_percentage_calculation(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress percentage calculation."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "progress_calc_test.pdf",
            metadata={'filename': 'progress_calc_test.pdf'}
        )
        
        # Act
        async with stage_tracker.start_stage(context, "progress_test") as stage_context:
            # Update progress at different percentages
            await stage_tracker.update_progress(stage_context, 25, "25% complete")
            await stage_tracker.update_progress(stage_context, 50, "50% complete")
            await stage_tracker.update_progress(stage_context, 75, "75% complete")
            await stage_tracker.update_progress(stage_context, 100, "100% complete")
        
        # Assert
        # Progress should have been updated at each step
        # In real implementation, verify progress values were saved correctly
    
    @pytest.mark.asyncio
    async def test_progress_with_custom_message(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress updates with custom messages."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "progress_message_test.pdf",
            metadata={'filename': 'progress_message_test.pdf'}
        )
        
        custom_messages = [
            "Starting text extraction...",
            "Extracting page 1 of 5...",
            "Extracting page 2 of 5...",
            "Extracting page 3 of 5...",
            "Extracting page 4 of 5...",
            "Extracting page 5 of 5...",
            "Text extraction complete."
        ]
        
        # Act
        async with stage_tracker.start_stage(context, "message_test") as stage_context:
            for i, message in enumerate(custom_messages):
                progress = (i + 1) * 100 // len(custom_messages)
                await stage_tracker.update_progress(stage_context, progress, message)
        
        # Assert
        # Custom messages should have been saved with progress
        # In real implementation, verify messages were preserved
    
    @pytest.mark.asyncio
    async def test_progress_out_of_bounds(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress updates with out-of-bounds values."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "progress_bounds_test.pdf",
            metadata={'filename': 'progress_bounds_test.pdf'}
        )
        
        # Act & Assert - Test various progress values
        async with stage_tracker.start_stage(context, "bounds_test") as stage_context:
            # Test negative progress (should be handled gracefully)
            await stage_tracker.update_progress(stage_context, -10, "Negative progress")
            
            # Test progress > 100 (should be handled gracefully)
            await stage_tracker.update_progress(stage_context, 150, "Progress > 100")
            
            # Test normal progress
            await stage_tracker.update_progress(stage_context, 50, "Normal progress")
            await stage_tracker.update_progress(stage_context, 100, "Complete")
        
        # Should handle out-of-bounds values gracefully
        # In real implementation, verify values were clamped or handled appropriately
    
    @pytest.mark.asyncio
    async def test_progress_with_zero_total(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress tracking when total work is zero."""
        # Arrange
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=100,
            chunk_overlap=20,
            stage_tracker=mock_stage_tracker
        )
        
        # Create empty content
        empty_file = temp_test_pdf / "empty_progress_test.pdf"
        empty_file.write_text("")
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=empty_file,
            metadata={'filename': empty_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        # Should handle zero work gracefully
        # Progress tracking should not fail even with no work to track
        # In real implementation, verify no progress tracking errors occurred
    
    @pytest.mark.asyncio
    async def test_progress_frequency_limiting(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test progress update frequency limiting."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "progress_frequency_test.pdf",
            metadata={'filename': 'progress_frequency_test.pdf'}
        )
        
        # Act
        update_count = 0
        
        async with stage_tracker.start_stage(context, "frequency_test") as stage_context:
            # Send many rapid progress updates
            for i in range(100):
                await stage_tracker.update_progress(stage_context, i, f"Update {i}")
                update_count += 1
        
        # Assert
        # Should handle rapid updates gracefully
        # In real implementation, verify frequency limiting worked if implemented
        assert update_count == 100, "All progress updates should be attempted"


class TestErrorHandling:
    """Test error handling in StageTracker integration."""
    
    @pytest.mark.asyncio
    async def test_stage_tracker_database_error(self, mock_database_adapter, temp_test_pdf, processor_test_config):
        """Test handling of database errors in stage tracking."""
        # Arrange
        # Mock database to fail
        async def failing_execute_rpc(rpc_name, params):
            raise Exception("Database connection failed")
        
        mock_database_adapter.execute_rpc = failing_execute_rpc
        
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "db_error_test.pdf",
            metadata={'filename': 'db_error_test.pdf'}
        )
        
        # Act
        try:
            async with stage_tracker.start_stage(context, "db_error_test") as stage_context:
                await stage_tracker.update_progress(stage_context, 50, "Progress during DB error")
        except Exception as e:
            # Should handle database errors gracefully
            assert "database" in str(e).lower() or "connection" in str(e).lower()
        
        # Assert
        # Should not crash the entire processing pipeline
        # In real implementation, verify graceful degradation
    
    @pytest.mark.asyncio
    async def test_stage_tracker_with_processor_failure(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test stage tracking when processor fails."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create file that will cause processing to fail
        corrupted_file = temp_test_pdf / "corrupted_for_tracker.pdf"
        corrupted_file.write_bytes(b"Invalid PDF content that causes failure")
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=corrupted_file,
            metadata={'filename': corrupted_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        assert not result.success, "Processing should fail"
        
        # Stage failure should be tracked
        # In real implementation, verify fail_stage was called with appropriate error
    
    @pytest.mark.asyncio
    async def test_partial_stage_failure_recovery(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test recovery from partial stage failures."""
        # Arrange
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=50,
            chunk_overlap=10,
            stage_tracker=mock_stage_tracker
        )
        
        # Create content
        test_content = " ".join([f"Partial failure test sentence {i}." for i in range(20)])
        
        test_file = temp_test_pdf / "partial_failure_test.pdf"
        test_file.write_text(test_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Mock partial failure during chunking
        original_chunk = processor.chunker.create_chunks
        call_count = 0
        
        async def failing_chunk(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 2:  # Fail on second call
                raise Exception("Temporary chunking failure")
            return await original_chunk(*args, **kwargs)
        
        with patch.object(processor.chunker, 'create_chunks', side_effect=failing_chunk):
            # Act
            result = await processor.process(context)
            
            # Assert
            # Should handle partial failure gracefully
            # In real implementation, verify appropriate error tracking
            if not result.success:
                assert "chunking" in result.error.lower() or "temporary" in result.error.lower()
    
    @pytest.mark.asyncio
    async def test_stage_tracker_timeout_handling(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test stage tracker timeout handling."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "timeout_test.pdf",
            metadata={'filename': 'timeout_test.pdf'}
        )
        
        # Act
        try:
            async with stage_tracker.start_stage(context, "timeout_test") as stage_context:
                # Simulate long-running operation
                await asyncio.sleep(0.1)  # Short sleep for test
                
                # Update progress
                await stage_tracker.update_progress(stage_context, 50, "Halfway through timeout test")
        
        except asyncio.TimeoutError:
            # Should handle timeouts gracefully
            pass
        
        # Assert
        # Should not crash on timeout
        # In real implementation, verify timeout handling
    
    @pytest.mark.asyncio
    async def test_concurrent_stage_tracking(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test concurrent stage tracking for multiple documents."""
        # Arrange
        processors = [
            UploadProcessor(
                database_adapter=mock_database_adapter,
                max_file_size_mb=processor_test_config['max_file_size_mb'],
                stage_tracker=mock_stage_tracker
            )
            for _ in range(3)
        ]
        
        # Create multiple test files
        test_files = []
        for i in range(3):
            content = f"Concurrent test document {i+1}. " * 20
            test_file = temp_test_pdf / f"concurrent_test_{i+1}.pdf"
            test_file.write_text(content)
            test_files.append(test_file)
        
        # Act - Process documents concurrently
        async def process_document(processor, file_path, doc_index):
            return await processor.process(file_path)
        
        tasks = [
            process_document(processors[i], test_files[i], i)
            for i in range(3)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Assert
        assert len(results) == 3, "Should have results for all documents"
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Should handle concurrent errors gracefully
                assert "concurrent" not in str(result).lower(), f"Document {i} should not have concurrency error"
            else:
                assert result.success, f"Document {i} should succeed"
                assert result.data['document_id'] is not None, f"Document {i} should have document ID"
    
    @pytest.mark.asyncio
    async def test_stage_tracker_invalid_context(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test stage tracker with invalid context."""
        # Arrange
        stage_tracker = StageTracker(mock_database_adapter)
        
        # Test with None context
        try:
            async with stage_tracker.start_stage(None, "invalid_test"):
                pass
        except (ValueError, AttributeError):
            # Should handle invalid context appropriately
            pass
        
        # Test with missing document_id
        invalid_context = ProcessingContext(
            document_id=None,  # Missing ID
            file_path=temp_test_pdf / "invalid_context.pdf",
            metadata={'filename': 'invalid_context.pdf'}
        )
        
        try:
            async with stage_tracker.start_stage(invalid_context, "invalid_test"):
                pass
        except (ValueError, AttributeError):
            # Should handle missing document ID
            pass


class TestDatabaseIntegration:
    """Test database integration for stage tracking."""
    
    @pytest.mark.asyncio
    async def test_stage_database_records(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test that stage records are created in database."""
        # Arrange
        processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        test_file = temp_test_pdf / "stage_db_test.pdf"
        test_file.write_text("Test content for database integration.")
        
        # Act
        result = await processor.process(test_file)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # In real implementation, verify stage records in database
        # For now, verify processing completed
        document_id = result.data['document_id']
        assert document_id is not None, "Should have document ID"
    
    @pytest.mark.asyncio
    async def test_progress_database_records(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test that progress records are saved to database."""
        # Arrange
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=50,
            chunk_overlap=10,
            stage_tracker=mock_stage_tracker
        )
        
        # Create content for progress tracking
        progress_content = " ".join([f"Progress test sentence {i}." for i in range(50)])
        
        test_file = temp_test_pdf / "progress_db_test.pdf"
        test_file.write_text(progress_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # In real implementation, verify progress records in database
        # For now, verify processing completed with chunks
        chunks = result.data['chunks']
        assert len(chunks) > 1, "Should create multiple chunks for progress tracking"
    
    @pytest.mark.asyncio
    async def test_stage_error_database_records(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test that stage errors are recorded in database."""
        # Arrange
        processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create corrupted file to trigger error
        corrupted_file = temp_test_pdf / "stage_error_db_test.pdf"
        corrupted_file.write_bytes(b"Invalid PDF content")
        
        # Act
        result = await processor.process(corrupted_file)
        
        # Assert
        assert not result.success, "Processing should fail"
        
        # In real implementation, verify error records in database
        # For now, verify error is captured
        assert result.error is not None, "Should have error message"
    
    @pytest.mark.asyncio
    async def test_stage_completion_database_records(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test that stage completion is recorded in database."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        test_content = """Test content for completion tracking.
This should complete successfully and be recorded in the database.

Technical specifications:
- Device: Test Device
- Status: Operational"""
        
        test_file = temp_test_pdf / "completion_db_test.pdf"
        test_file.write_text(test_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act
        result = await processor.process(context)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # In real implementation, verify completion records in database
        # For now, verify successful processing
        assert 'page_texts' in result.data, "Should have extracted page texts"
        assert 'metadata' in result.data, "Should have document metadata"
    
    @pytest.mark.asyncio
    async def test_concurrent_stage_database_records(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test concurrent stage record creation in database."""
        # Arrange
        processors = [
            UploadProcessor(
                database_adapter=mock_database_adapter,
                max_file_size_mb=processor_test_config['max_file_size_mb'],
                stage_tracker=mock_stage_tracker
            )
            for _ in range(3)
        ]
        
        # Create multiple test files
        test_files = []
        for i in range(3):
            content = f"Concurrent DB test document {i+1}. " * 15
            test_file = temp_test_pdf / f"concurrent_db_test_{i+1}.pdf"
            test_file.write_text(content)
            test_files.append(test_file)
        
        # Act - Process concurrently
        async def process_document(processor, file_path):
            return await processor.process(file_path)
        
        tasks = [process_document(processors[i], test_files[i]) for i in range(3)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Assert
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 2, "Most documents should succeed"
        
        # In real implementation, verify all stage records are in database
        # For now, verify document IDs are unique
        document_ids = [r.data['document_id'] for r in successful_results]
        assert len(set(document_ids)) == len(document_ids), "Document IDs should be unique"


class TestContextManagement:
    """Test context management with stage tracking."""
    
    @pytest.mark.asyncio
    async def test_context_preservation_with_tracking(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test that context is preserved during stage tracking."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create context with metadata
        original_context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "context_preservation_test.pdf",
            metadata={
                'filename': 'context_preservation_test.pdf',
                'manufacturer': 'TestCorp',
                'model': 'C4080',
                'document_type': 'service_manual'
            }
        )
        
        # Add content to file
        original_context.file_path.write_text("Test content for context preservation.")
        
        # Act
        result = await processor.process(original_context)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # Context should be preserved and potentially enriched
        # In real implementation, verify context metadata is preserved
        document_metadata = result.data['metadata']
        assert document_metadata.get('manufacturer') == 'TestCorp', "Should preserve manufacturer"
        assert document_metadata.get('model') == 'C4080', "Should preserve model"
        assert document_metadata.get('document_type') == 'service_manual', "Should preserve document type"
    
    @pytest.mark.asyncio
    async def test_context_enrichment_during_tracking(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test context enrichment during stage tracking."""
        # Arrange
        processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create minimal context
        minimal_context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "context_enrichment_test.pdf",
            metadata={'filename': 'context_enrichment_test.pdf'}
        )
        
        # Add rich content for enrichment
        rich_content = """Konica Minolta C750i Service Manual
=====================================

This document contains technical information for enrichment testing.
The processor should detect and extract metadata.

Error Codes:
900.01: Fuser Unit Error
900.02: Exposure Lamp Error

Technical Specifications:
- Print Speed: 75 ppm
- Resolution: 1200 x 1200 dpi"""
        
        minimal_context.file_path.write_text(rich_content)
        
        # Act
        result = await processor.process(minimal_context)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # Context should be enriched with detected information
        document_metadata = result.data['metadata']
        assert 'manufacturer' in document_metadata, "Should detect and add manufacturer"
        assert 'language' in document_metadata, "Should detect and add language"
        assert 'document_type' in document_metadata, "Should detect and add document type"
    
    @pytest.mark.asyncio
    async def test_context_propagation_across_stages(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test context propagation across multiple stages with tracking."""
        # Arrange
        upload_processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        document_processor = DocumentProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            stage_tracker=mock_stage_tracker
        )
        
        text_processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            chunk_size=processor_test_config['chunk_size'],
            chunk_overlap=processor_test_config['chunk_overlap'],
            stage_tracker=mock_stage_tracker
        )
        
        # Create initial context
        initial_context = ProcessingContext(
            document_id="test-doc-id",
            file_path=temp_test_pdf / "context_propagation_test.pdf",
            metadata={'filename': 'context_propagation_test.pdf'}
        )
        
        initial_context.file_path.write_text("""Test content for context propagation.
This content should flow through all stages.
Each stage should preserve and enrich the context.

Technical details:
- Device: Test Device X
- Version: 2.1
- Status: Ready""")
        
        # Act - Process through all stages
        # Stage 1: Upload
        upload_result = await upload_processor.process(initial_context.file_path)
        assert upload_result.success, "Upload should succeed"
        
        # Stage 2: Document Processing
        document_context = ProcessingContext(
            document_id=upload_result.data['document_id'],
            file_path=initial_context.file_path,
            metadata=upload_result.metadata
        )
        
        document_result = await document_processor.process(document_context)
        assert document_result.success, "Document processing should succeed"
        
        # Stage 3: Text Processing
        text_context = ProcessingContext(
            document_id=document_result.data.get('metadata', {}).get('document_id'),
            file_path=initial_context.file_path,
            metadata=document_result.data['metadata']
        )
        text_context.page_texts = document_result.data['page_texts']
        
        text_result = await text_processor.process(text_context)
        assert text_result.success, "Text processing should succeed"
        
        # Assert
        # Context should be propagated and enriched through all stages
        final_context = text_result.data.get('context')
        assert final_context is not None, "Should return final context"
        
        # Verify original metadata is preserved
        final_metadata = final_context.metadata
        assert final_metadata.get('filename') == 'context_propagation_test.pdf', "Should preserve filename"
        
        # Verify enriched metadata
        assert 'manufacturer' in final_metadata, "Should have manufacturer from document processing"
        assert 'document_type' in final_metadata, "Should have document type from document processing"
    
    @pytest.mark.asyncio
    async def test_context_with_stage_tracking_data(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test context includes stage tracking information."""
        # Arrange
        processor = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        test_file = temp_test_pdf / "stage_tracking_context_test.pdf"
        test_file.write_text("Test content for stage tracking context test.")
        
        # Act
        result = await processor.process(test_file)
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # Context might include stage tracking information
        # In real implementation, verify stage tracking data is included
        document_id = result.data['document_id']
        assert document_id is not None, "Should have document ID"
        
        # Stage tracking metadata might be included
        stage_metadata = result.metadata.get('stage_tracking', {})
        # Verify stage tracking information if implemented


class TestPerformance:
    """Test performance characteristics of stage tracking."""
    
    @pytest.mark.asyncio
    async def test_stage_tracking_overhead(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test performance overhead of stage tracking."""
        # Arrange
        # Create processors with and without stage tracking
        processor_with_tracking = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        
        processor_without_tracking = UploadProcessor(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=None
        )
        
        test_file = temp_test_pdf / "performance_overhead_test.pdf"
        test_file.write_text("Test content for performance overhead testing.")
        
        # Act - Measure processing times
        start_time = time.time()
        result_with_tracking = await processor_with_tracking.process(test_file)
        time_with_tracking = time.time() - start_time
        
        start_time = time.time()
        result_without_tracking = await processor_without_tracking.process(test_file)
        time_without_tracking = time.time() - start_time
        
        # Assert
        assert result_with_tracking.success, "Processing with tracking should succeed"
        assert result_without_tracking.success, "Processing without tracking should succeed"
        
        # Stage tracking overhead should be reasonable
        overhead_ratio = time_with_tracking / time_without_tracking if time_without_tracking > 0 else 1
        assert overhead_ratio < 5.0, f"Stage tracking overhead should be reasonable, ratio: {overhead_ratio}"
        
        print(f"Performance overhead test:")
        print(f"  With tracking: {time_with_tracking:.4f}s")
        print(f"  Without tracking: {time_without_tracking:.4f}s")
        print(f"  Overhead ratio: {overhead_ratio:.2f}x")
    
    @pytest.mark.asyncio
    async def test_progress_update_frequency_performance(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test performance impact of frequent progress updates."""
        # Arrange
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=25,  # Small chunks for more frequent updates
            chunk_overlap=5,
            stage_tracker=mock_stage_tracker
        )
        
        # Create content that will trigger many progress updates
        frequent_content = " ".join([f"Frequent progress test sentence {i}." for i in range(100)])
        
        test_file = temp_test_pdf / "progress_frequency_performance_test.pdf"
        test_file.write_text(frequent_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act - Measure processing time with frequent progress updates
        start_time = time.time()
        result = await processor.process(context)
        processing_time = time.time() - start_time
        
        # Assert
        assert result.success, "Processing should succeed"
        
        chunks = result.data['chunks']
        assert len(chunks) > 5, "Should create multiple chunks for progress testing"
        
        # Processing time should be reasonable despite frequent updates
        assert processing_time < 30.0, f"Frequent progress updates should not cause excessive delay: {processing_time:.2f}s"
        
        print(f"Progress frequency performance:")
        print(f"  Processing time: {processing_time:.4f}s")
        print(f"  Chunks created: {len(chunks)}")
        print(f"  Time per chunk: {processing_time/len(chunks):.4f}s")
    
    @pytest.mark.asyncio
    async def test_concurrent_stage_tracking_performance(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test performance of concurrent stage tracking."""
        # Arrange
        processors = [
            UploadProcessor(
                database_adapter=mock_database_adapter,
                max_file_size_mb=processor_test_config['max_file_size_mb'],
                stage_tracker=mock_stage_tracker
            )
            for _ in range(5)
        ]
        
        # Create multiple test files
        test_files = []
        for i in range(5):
            content = f"Concurrent performance test document {i+1}. " * 20
            test_file = temp_test_pdf / f"concurrent_perf_test_{i+1}.pdf"
            test_file.write_text(content)
            test_files.append(test_file)
        
        # Act - Measure concurrent processing time
        start_time = time.time()
        
        async def process_document(processor, file_path):
            return await processor.process(file_path)
        
        tasks = [process_document(processors[i], test_files[i]) for i in range(5)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        concurrent_time = time.time() - start_time
        
        # Also measure sequential time for comparison
        start_time = time.time()
        for i in range(5):
            await processors[i].process(test_files[i])
        sequential_time = time.time() - start_time
        
        # Assert
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 4, "Most documents should succeed"
        
        # Concurrent processing should be faster than sequential
        speedup_ratio = sequential_time / concurrent_time if concurrent_time > 0 else 1
        assert speedup_ratio > 1.5, f"Concurrent processing should be significantly faster: {speedup_ratio:.2f}x"
        
        print(f"Concurrent stage tracking performance:")
        print(f"  Concurrent time: {concurrent_time:.4f}s")
        print(f"  Sequential time: {sequential_time:.4f}s")
        print(f"  Speedup ratio: {speedup_ratio:.2f}x")
    
    @pytest.mark.asyncio
    async def test_stage_tracking_memory_usage(self, mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker):
        """Test memory usage of stage tracking."""
        # Arrange
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        processor = OptimizedTextProcessor(
            database_adapter=mock_database_adapter,
            chunk_size=50,
            chunk_overlap=10,
            stage_tracker=mock_stage_tracker
        )
        
        # Create content that will generate many stage tracking events
        memory_test_content = " ".join([f"Memory test sentence {i} with stage tracking." for i in range(200)])
        
        test_file = temp_test_pdf / "memory_usage_stage_test.pdf"
        test_file.write_text(memory_test_content)
        
        context = ProcessingContext(
            document_id="test-doc-id",
            file_path=test_file,
            metadata={'filename': test_file.name}
        )
        
        # Act - Process with stage tracking
        result = await processor.process(context)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # Assert
        assert result.success, "Processing should succeed"
        
        # Memory usage should be reasonable
        assert memory_increase < 100, f"Stage tracking memory increase should be reasonable: {memory_increase:.2f}MB"
        
        chunks = result.data['chunks']
        assert len(chunks) > 1, "Should create multiple chunks"
        
        print(f"Stage tracking memory usage:")
        print(f"  Initial memory: {initial_memory:.2f}MB")
        print(f"  Final memory: {final_memory:.2f}MB")
        print(f"  Memory increase: {memory_increase:.2f}MB")
        print(f"  Chunks created: {len(chunks)}")


# Parameterized tests for different stage types
@pytest.mark.parametrize("stage_type,processor_class,test_content", [
    ("upload", UploadProcessor, "Test content for upload stage tracking."),
    ("document", DocumentProcessor, "Document stage content for tracking.\nTechnical specifications.\nError codes: 900.01."),
    ("text", OptimizedTextProcessor, "Text stage content. " * 20),
])
@pytest.mark.asyncio
async def test_stage_type_tracking(mock_database_adapter, temp_test_pdf, processor_test_config, mock_stage_tracker, stage_type, processor_class, test_content):
    """Test tracking for different stage types."""
    # Arrange
    if stage_type == "upload":
        processor = processor_class(
            database_adapter=mock_database_adapter,
            max_file_size_mb=processor_test_config['max_file_size_mb'],
            stage_tracker=mock_stage_tracker
        )
        test_file = temp_test_pdf / f"{stage_type}_stage_test.pdf"
        test_file.write_text(test_content)
        
        # Act
        result = await processor.process(test_file)
        
    else:
        processor = processor_class(
            database_adapter=mock_database_adapter,
            pdf_engine=processor_test_config['pdf_engine'],
            chunk_size=processor_test_config['chunk_size'],
            chunk_overlap=processor_test_config['chunk_overlap'],
            stage_tracker=mock_stage_tracker
        )
        
        test_file = temp_test_pdf / f"{stage_type}_stage_test.pdf"
        test_file.write_text(test_content)
        
        context = ProcessingContext(
            document_id=f"test-doc-{stage_type}",
            file_path=test_file,
            metadata={'filename': f"{stage_type}_stage_test.pdf"}
        )
        
        # Act
        result = await processor.process(context)
    
    # Assert
    assert result.success, f"{stage_type} stage should succeed"
    
    # Stage tracking should have been performed
    # In real implementation, verify appropriate stage tracking calls
    if stage_type == "upload":
        assert result.data['document_id'] is not None, "Upload should create document"
    elif stage_type == "document":
        assert 'page_texts' in result.data, "Document should extract page texts"
        assert 'metadata' in result.data, "Document should extract metadata"
    elif stage_type == "text":
        assert 'chunks' in result.data, "Text should create chunks"
        assert len(result.data['chunks']) > 0, "Text should create at least one chunk"


if __name__ == "__main__":
    # Allow running tests directly
    pytest.main([__file__, "-v"])
