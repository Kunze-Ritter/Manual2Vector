    # ===========================================
    # SUPABASE CLOUD CONFIGURATION
    # ===========================================
    SUPABASE_URL=https://crujfdpqdjzcfqeyhang.supabase.co
    SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNydWpmZHBxZGp6Y2ZxZXloYW5nIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTkwNDY1MTUsImV4cCI6MjA3NDYyMjUxNX0.kDSf9jMYbNgzV8v1f-_kSoSy_cAMFLG367m9ZbDsBkw
    SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNydWpmZHBxZGp6Y2ZxZXloYW5nIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1OTA0NjUxNSwiZXhwIjoyMDc0NjIyNTE1fQ.5MnFW5MuKdS6ZNvKv5iTWH-jv_ZB1SgeoP7cVGI7cdE
    SUPABASE_STORAGE_URL=https://crujfdpqdjzcfqeyhang.supabase.co/storage/v1

    # Database Connection (f√ºr direkte PostgreSQL Verbindung)
    DATABASE_CONNECTION_URL=postgresql://postgres:YOUR_DB_PASSWORD@db.crujfdpqdjzcfqeyhang.supabase.co:5432/postgres
    DATABASE_PASSWORD=yoMHeJeFTle8LKL7

    # ===========================================
    # R2 STORAGE CONFIGURATION (Cloudflare)
    # ===========================================
    R2_ACCESS_KEY_ID=9c59473961632448c91db3ef9dbd35ab
    R2_SECRET_ACCESS_KEY=9cc62a9506ac9ec6e8373a39fa86268bc187632e5548e8c37b1c6c9c071755e4
    R2_BUCKET_NAME_DOCUMENTS=krai-documents-images
    R2_ENDPOINT_URL=https://a88f92c913c232559845adb9001a5d14.eu.r2.cloudflarestorage.com
    R2_REGION=auto

    # ===========================================
    # R2 PUBLIC URLS
    # ===========================================
    R2_PUBLIC_URL_DOCUMENTS=https://pub-68e63cf2d6ac4222adaab70dfbc29ec4.r2.dev
    R2_PUBLIC_URL_ERROR=https://pub-e327cb3371c741e08c5e8672e817d9cf.r2.dev
    R2_PUBLIC_URL_PARTS=https://pub-61c8b15e7bf24febbf8e0197ab237041.r2.dev

    # ===========================================
    # OLLAMA AI SERVICE CONFIGURATION
    # ===========================================
    OLLAMA_URL=http://localhost:11434
    OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest
    OLLAMA_MODEL_TEXT=llama3.2:latest
    
    # Vision Model - Auto-detected based on GPU VRAM
    # Override here if you want a specific model
    # bakllava:7b = 4GB VRAM (recommended - more stable than llava)
    # llava:7b = 4GB VRAM (alternative)
    # llava:latest = 11GB VRAM (for 12GB+ GPU)
    OLLAMA_MODEL_VISION=llava-phi3:latest
    
    # Disable vision processing if model keeps crashing
    # Set to 'true' to skip AI image analysis (faster, uses less VRAM)
    # The pipeline will still extract images but won't analyze them with AI
    DISABLE_VISION_PROCESSING=false
    
    # Maximum number of images to process with vision model per document
    # Lower values = more stable, higher values = more complete analysis
    # Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
    MAX_VISION_IMAGES=5