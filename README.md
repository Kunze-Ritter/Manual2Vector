# ğŸš€ KR-AI-Engine - Production Ready

**Complete AI-Powered Document Processing Pipeline with 8-Stage Architecture**

[![Version](https://img.shields.io/badge/version-3.0.0-blue.svg)](https://github.com/Kunze-Ritter/KR-AI-Engine)
[![Architecture](https://img.shields.io/badge/architecture-8--stage-purple.svg)](#architecture)
[![Cloud](https://img.shields.io/badge/cloud-supabase-green.svg)](https://supabase.com)
[![Storage](https://img.shields.io/badge/storage-cloudflare%20r2-orange.svg)](https://cloudflare.com)
[![AI](https://img.shields.io/badge/ai-ollama-ff69b4.svg)](https://ollama.ai)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Organization](https://img.shields.io/badge/org-Kunze%20&%20Ritter-blue.svg)](https://github.com/Kunze-Ritter)

> **Production-ready AI document processing system with complete 8-stage pipeline for Kunze & Ritter GmbH**

---

## ğŸ“‹ Inhaltsverzeichnis

- [ğŸ¯ Ãœberblick](#Ã¼berblick)
- [âœ¨ Features](#features)  
- [ğŸ—ï¸ Architektur](#architektur)
- [ğŸš€ Quick Start](#quick-start)
- [ğŸ“– Dokumentation](#dokumentation)
- [ğŸ› ï¸ Entwicklung](#entwicklung)
- [ğŸ¤ Contributing](#contributing)

---

## ğŸ¯ Ãœberblick

KR-AI-Engine ist ein vollstÃ¤ndiges, production-ready KI-gestÃ¼tztes Dokumentenverarbeitungssystem mit 8-stufiger Pipeline-Architektur. Es kombiniert Supabase Cloud, Ollama AI und Cloudflare R2 fÃ¼r eine hochperformante Dokumentenverarbeitung mit intelligenter Klassifizierung und Semantic Search.

### ğŸ¯ Hauptziele (Production Setup)

- **ğŸ—ï¸ 8-Stage Pipeline** - Komplette Verarbeitungskette von Upload bis Search
- **â˜ï¸ Cloud-First** - Supabase Cloud, Cloudflare R2, Ollama AI
- **ğŸ” AI-Powered Classification** - Automatische Hersteller-/Modell-Erkennung
- **ğŸ–¼ï¸ Advanced Image Processing** - OCR, AI Vision, Format-Erhaltung
- **ğŸ”® Semantic Search** - Vector-basierte Suche mit EmbeddingGemma
- **ğŸ’¾ Intelligent Storage** - Deduplication, R2 Integration, Database-only Documents
- **ğŸ“Š Real-time Progress** - Advanced Progress Tracking mit ETA

---

## âœ¨ Features

### ğŸ—ï¸ **8-Stage Processing Pipeline**
- **Stage 1: Upload Processor** - Document validation, hash generation, database storage
- **Stage 2: Text Processor** - PDF text extraction, intelligent chunking
- **Stage 3: Image Processor** - Image extraction, OCR, AI vision analysis
- **Stage 4: Classification Processor** - AI-powered manufacturer/model detection
- **Stage 5: Metadata Processor** - Error codes, version extraction
- **Stage 6: Storage Processor** - R2 object storage, deduplication
- **Stage 7: Embedding Processor** - Vector embedding generation
- **Stage 8: Search Processor** - Search index, analytics

### ğŸ¯ **AI-Powered Intelligence**
- **Manufacturer Normalization** - HP, HP Inc. â†’ HP Inc. (prevents duplicates)
- **Model Detection** - Extracts ALL models from documents (not just filename)
- **Document Classification** - Service manual, parts catalog, user manual detection
- **Vision AI** - OCR + AI analysis with Ollama llava model
- **Semantic Search** - Vector embeddings with embeddinggemma model

### ğŸ”— **Advanced Link Management**
- **PDF Page Mapping** - Automatische Zuordnung Adobe â†” Actual Seitennummern
- **Link Deduplication** - Intelligente Duplikatserkennung mit Multi-Document References
- **Video Integration** - Automatische Integration in `instructional_videos` Tabelle
- **Cross-Document References** - Links mit Verweisen auf alle Fundstellen

### ğŸ§  **AI & Machine Learning**
- **Multi-Model Support** - Llama3.2, EmbeddingGemma, LLaVA Vision
- **Advanced Chunking** - 5 verschiedene Chunking-Strategien
- **Vector Search** - 768-dimensionale Embeddings mit HNSW-Index
- **Vision AI** - OCR, Diagrammanalyse, Defekterkennung

### ğŸ“„ **Dokumentenverarbeitung**
- **PDF Intelligence** - Text, Bilder, Metadaten-Extraktion
- **Hybrid Classification** - Filename + Content + LLM Analysis
- **Model Expansion** - Automatische Erweiterung von Modell-Platzhaltern
- **Version Detection** - Intelligente Versionserkennung

### ğŸ—„ï¸ **Enterprise Storage**
- **Supabase Integration** - PostgreSQL + Vector Extensions
- **Smart Deduplication** - Hash-basierte Duplikatserkennung
- **Image Routing** - Intelligente Speicherung nach Bildtyp
- **Audit Logging** - VollstÃ¤ndige Verarbeitungshistorie

---

## ğŸ—ï¸ Architektur

### ğŸ­ **Modular Processing System**

```mermaid
graph TB
    A[ğŸ“„ Document Input] --> B[ğŸ­ ModularDocumentProcessor]
    B --> C[ğŸ“ TextProcessor]
    B --> D[ğŸ–¼ï¸ ImageProcessor]  
    B --> E[ğŸ·ï¸ ClassificationProcessor]
    B --> F[ğŸ”® EmbeddingProcessor]
    B --> G[ğŸ’¾ StorageProcessor]
    
    C --> H[ğŸ“Š Processing Results]
    D --> H
    E --> H  
    F --> H
    G --> H
```

### ğŸ”§ **Core Components**

| Component | Beschreibung | Funktionen |
|-----------|--------------|------------|
| **ğŸ“ TextProcessor** | PDF-Textextraktion & Chunking | PyMuPDF, 5 Chunking-Strategien, Struktur-Analyse |
| **ğŸ–¼ï¸ ImageProcessor** | Bildverarbeitung & Vision AI | OCR, LLaVA Vision, Bildoptimierung |
| **ğŸ·ï¸ ClassificationProcessor** | Dokument-Klassifizierung | Hybrid-Ansatz, LLM-Integration |
| **ğŸ”® EmbeddingProcessor** | Vector-Generierung | EmbeddingGemma, Batch-Processing, Deduplication |
| **ğŸ’¾ StorageProcessor** | Datenbank & Storage | Supabase, Hash-Deduplication, Smart Routing |

### ğŸ“Š **9-Stage Processing Pipeline**

1. **ğŸ“¤ Upload** - Dokumenten-Upload und Validierung
2. **ğŸ“„ Text Extraction** - PDF-Text und Struktur-Analyse  
3. **ğŸ–¼ï¸ Image Processing** - Bildextraktion und OCR
4. **ğŸ·ï¸ Document Classification** - Typ, Hersteller, Modell-Erkennung
5. **ğŸ“‘ Metadata Extraction** - Version, Serie, Zusatzinfos
6. **ğŸ’¾ Document Storage** - PrimÃ¤re Datenspeicherung
7. **ğŸ”ª Text Chunking** - Intelligente Textaufteilung
8. **ğŸ”® Embedding Generation** - Vector-Erstellung fÃ¼r Semantic Search
9. **âœ… Finalization** - Abschluss und Statusupdate

---

## ğŸš€ Quick Start (Minimal Setup)

### ğŸ“‹ Voraussetzungen

- **Python** (v3.9+)
- **Git** (v2.30+)  
- **Ollama Windows App** (installiert und laufend)
- **Supabase Cloud Account** (kostenlos)
- **Cloudflare R2 Account** (optional)
- **4GB+ RAM** empfohlen
- **5GB+ freier Speicher**

### âš¡ Minimal Installation

#### ğŸ¯ **Schritt 1: Repository klonen**
```bash
git clone https://github.com/Kunze-Ritter/KR-AI-Engine.git
cd KR-AI-Engine
```

> **ğŸ’¡ Hinweis**: Das Repository ist bereits mit einer fertigen `.env`-Datei konfiguriert! Sie kÃ¶nnen direkt loslegen.

#### ğŸ¯ **Schritt 2: Environment konfigurieren**
```bash
# .env Datei ist bereits vorhanden und konfiguriert!
# Falls nÃ¶tig, kÃ¶nnen Sie die Werte anpassen:
# - Supabase URL, API Keys
# - Cloudflare R2 Credentials  
# - Ollama Model Settings
```

> **âœ… Bereits konfiguriert**: Die `.env`-Datei enthÃ¤lt bereits alle notwendigen Cloud-Credentials fÃ¼r sofortigen Start!

#### ğŸ¯ **Schritt 3: Dependencies installieren**
```bash
# Minimal requirements installieren
pip install -r backend/requirements.txt
```

#### ğŸ¯ **Schritt 4: Ollama Windows App starten**
```bash
# Ollama Windows App starten (manuell)
# Models herunterladen: llama3.2:3b, embeddinggemma, llava:7b
```

#### ğŸ¯ **Schritt 5: System starten**
```bash
# Backend starten
cd backend
python main.py

# Oder mit PowerShell (Windows)
./start_krai.ps1
```

### ğŸ§ª System testen

#### **Cloud Services:**
```bash
# Supabase Connection testen
curl https://your-project.supabase.co/rest/v1/

# Ollama Windows App testen
curl http://localhost:11434/api/tags

# KRAI API testen
curl http://localhost:8002/health
```

### ğŸ“„ Erstes Dokument verarbeiten

```bash
# Dokument Ã¼ber KRAI API verarbeiten
curl -X POST "http://localhost:8002/api/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your-service-manual.pdf"

# Oder direkt Ã¼ber Python Script
python backend/krai_main.py --demo your-document.pdf
```

---

## ğŸ“– Dokumentation

### ğŸš€ **Migration & Deployment**

- **[ğŸ“‹ Migration Guide](migration/README.md)** - Alle Migration-Tools und Anleitungen
- **[ğŸªŸ Windows Migration](migration/documentation/WINDOWS_MIGRATION_GUIDE.md)** - Windows Laptop Setup
- **[ğŸ–¥ï¸ Server Migration](migration/documentation/DOCKER_PORTABILITY.md)** - Server-Migration Details
- **[âš™ï¸ Deployment Options](migration/documentation/SERVER_DEPLOYMENT_OPTIONS.md)** - Alle Deployment-Optionen

### ğŸ“š **System Dokumentation**

- **[ğŸ—ï¸ Architektur-Guide](documentation/ARCHITECTURE.md)** - Detaillierte System-Architektur
- **[ğŸ”§ API-Dokumentation](documentation/API.md)** - VollstÃ¤ndige API-Referenz
- **[ğŸ­ Module-Dokumentation](documentation/MODULES.md)** - Prozessor-Module im Detail
- **[ğŸš€ Deployment-Guide](documentation/DEPLOYMENT.md)** - Production Deployment
- **[âš™ï¸ Konfiguration](documentation/CONFIGURATION.md)** - System-Konfiguration
- **[ğŸ§ª Testing-Guide](documentation/TESTING.md)** - Tests und Validierung
- **[ğŸ“Š Performance](documentation/PERFORMANCE.md)** - Performance-Optimierung
- **[ğŸ’¾ Backup & Migration](documentation/BACKUP_AND_MIGRATION.md)** - Backup, Restore und Migration

### ğŸ”— **Quick Links (Minimal Setup)**

#### **Cloud Services:**
- **Supabase Studio**: https://supabase.com/dashboard (Database Management)
- **Supabase API**: https://your-project.supabase.co/rest/v1/ (Database API)
- **Cloudflare R2**: https://dash.cloudflare.com (Object Storage)

#### **Local Services:**
- **KRAI API**: http://localhost:8002 (Document Processing API)
- **Ollama**: http://localhost:11434 (AI Models - Windows App)
- **Supabase MCP**: Cursor Integration (Database Connection)

---

## ğŸ› ï¸ Entwicklung

### ğŸ­ **Development Setup (Minimal)**

```bash
# Environment konfigurieren
cp .env.example .env
# .env mit Cloud-Credentials anpassen

# Dependencies installieren
pip install -r backend/requirements.txt

# Ollama Windows App starten
# Models herunterladen: llama3.2:3b, embeddinggemma, llava:7b

# Backend starten
cd backend
python main.py

# Logs verfolgen
# Backend Logs: Console Output
# Ollama Logs: Windows App Interface
# Supabase Logs: Cloud Dashboard
```

### ğŸ”§ **Module entwickeln**

```python
# Neuen Processor erstellen
from modules.interfaces.base_processor import BaseProcessor

class CustomProcessor(BaseProcessor):
    async def process(self, data, context):
        # Your processing logic here
        return ProcessingResult(success=True, data=result)
```

### ğŸ“Š **Metriken & Monitoring (Minimal)**

```bash
# Processor-Metriken abrufen
curl http://localhost:8002/api/documents/stats/processing

# Live Processing Status  
curl http://localhost:8002/api/documents/sessions/active

# Supabase Database Stats (Ã¼ber MCP)
# Cursor â†’ Supabase MCP â†’ Database Queries

# Ollama Model Status
curl http://localhost:11434/api/tags
```

---

## ğŸ¢ **Produktions-Deployment (Minimal)**

### â˜ï¸ **Cloud-First Deployment**

```bash
# Environment fÃ¼r Production konfigurieren
cp .env.example .env.production

# Production Credentials eintragen:
# - Supabase Production URL & Keys
# - Cloudflare R2 Production Credentials
# - Ollama Production Models

# Backend als Service starten
python backend/main.py --production
```

### ğŸ”’ **Security Checklist (Minimal)**

- âœ… Supabase Cloud Security aktiviert
- âœ… Cloudflare R2 Access Keys sicher gespeichert
- âœ… Ollama Windows App sicher konfiguriert
- âœ… Environment-Variablen verschlÃ¼sselt
- âœ… Supabase MCP sichere Verbindung

### ğŸ“ˆ **Skalierung (Cloud-First)**

- **Horizontal**: Mehrere Backend-Instanzen
- **Vertical**: CPU/Memory pro Instanz erhÃ¶hen
- **Database**: Supabase Cloud Auto-Scaling
- **Storage**: Cloudflare R2 Global CDN
- **AI**: Ollama Model Caching

---

## ğŸ¤ Contributing

Wir freuen uns Ã¼ber Contributions! Bitte beachten Sie:

1. **Fork** das Repository
2. **Feature Branch** erstellen (`git checkout -b feature/amazing-feature`)
3. **Commit** Ihre Ã„nderungen (`git commit -m 'Add amazing feature'`)
4. **Push** zum Branch (`git push origin feature/amazing-feature`)
5. **Pull Request** Ã¶ffnen

### ğŸ“ **Code Standards**

- **Python**: PEP 8, Type Hints, Async/Await
- **Docker**: Multi-stage builds, Security best practices
- **Tests**: Pytest, 80%+ Coverage
- **Documentation**: Inline comments, README updates

---

## ğŸ“„ **License**

Dieses Projekt steht unter der [MIT License](LICENSE).

```
Copyright (c) 2025 Kunze & Ritter GmbH

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files...
```

---

## ğŸ¢ **Ãœber Kunze & Ritter GmbH**

**Kunze & Ritter GmbH** ist spezialisiert auf innovative KI-LÃ¶sungen fÃ¼r technische Serviceumgebungen. Mit jahrzehntelanger Erfahrung in der Druckertechnik entwickeln wir cutting-edge LÃ¶sungen fÃ¼r die Zukunft des technischen Services.

---

## ğŸ“ **Support & Kontakt**

- **ğŸ“§ Email**: support@kunze-ritter.de
- **ğŸŒ Website**: https://kunze-ritter.de
- **ğŸ“± Issues**: [GitHub Issues](https://github.com/your-org/kr-ai-engine/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/your-org/kr-ai-engine/discussions)

---

<div align="center">

**ğŸš€ Powered by Kunze & Ritter GmbH | Built with â¤ï¸ for Technical Service Excellence**

</div>