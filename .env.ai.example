# ===========================================
# AI SERVICE CONFIGURATION (Ollama)
# ===========================================

# Ollama Server URL
# Use host.docker.internal if Ollama runs in Docker
# Use localhost:11434 if Ollama runs locally
OLLAMA_URL=http://localhost:11434

# AI Models - Specialized for different tasks
# ================================================

# Embedding Model (Vector Generation)
# Used for: Semantic search, similarity matching
# Recommended: nomic-embed-text (fast, accurate, 768-dim)
OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest

# Structured Extraction Model (JSON, Products, Specs)
# Used for: Product extraction, specification parsing
# Recommended: qwen2.5:3b (excellent at JSON/structured output)
OLLAMA_MODEL_EXTRACTION=qwen2.5:3b

# Conversational AI Model (Agent, Chat, Summaries)
# Used for: User chat, natural language responses
# Recommended: llama3.2:3b (better at conversation/reasoning)
OLLAMA_MODEL_CHAT=llama3.2:3b

# Vision Model (Image Analysis)
# Used for: Diagram analysis, OCR, image understanding
# Recommended: llava:7b (4GB VRAM) or llava:13b (8GB+ VRAM)
OLLAMA_MODEL_VISION=llava:7b

# Legacy Model (Fallback for old code)
# Will be removed in future versions
OLLAMA_MODEL_TEXT=qwen2.5:3b

# Ollama Context Length (tokens)
# Lower = less VRAM usage, higher = longer conversations
# 8192 = ~1GB VRAM, 32768 = ~3.5GB VRAM, 131072 = ~7GB VRAM
# Recommended: 8192 for 8GB GPU, 32768 for 12GB+ GPU
OLLAMA_NUM_CTX=8192

# Vision Processing Settings
# Set to 'true' to skip AI image analysis (faster, uses less VRAM)
# The pipeline will still extract images but won't analyze them with AI
DISABLE_VISION_PROCESSING=false

# LLM Product Extraction - Maximum pages to scan
# Default: 50 (scans first 50 pages for product info)
# Set to 0 to scan ALL pages (slow but complete)
# Set to -1 to DISABLE LLM extraction (use pattern matching only)
# Why limit? LLM is slow (~15s/chunk). Large docs = long processing!
# Product specs are usually in first 50 pages anyway.
LLM_MAX_PAGES=50

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5

# ===========================================
# GPU/CPU CONFIGURATION
# ===========================================
# Enable GPU acceleration for OpenCV and ML models
# Set to 'true' to use GPU (requires CUDA and opencv-contrib-python)
# Set to 'false' to use CPU only (default, works everywhere)
USE_GPU=false

# CUDA device to use (if USE_GPU=true)
# 0 = first GPU, 1 = second GPU, etc.
CUDA_VISIBLE_DEVICES=0
