# ===========================================
# AI SERVICE CONFIGURATION (Ollama)
# ===========================================

# Ollama Server URL
# Use host.docker.internal if Ollama runs in Docker
# Use localhost:11434 if Ollama runs locally
OLLAMA_URL=http://localhost:11434

# AI Models
OLLAMA_MODEL_EMBEDDING=nomic-embed-text:latest
OLLAMA_MODEL_TEXT=llama3.2:3b
OLLAMA_MODEL_VISION=llava:7b

# Agent Chat Model (optional - defaults to OLLAMA_MODEL_TEXT)
# Use llama3.2 for better conversational responses
# Use qwen2.5 for better structured/technical responses
OLLAMA_MODEL_CHAT=llama3.2:3b

# Ollama Context Length (tokens)
# Lower = less VRAM usage, higher = longer conversations
# 8192 = ~1GB VRAM, 32768 = ~3.5GB VRAM, 131072 = ~7GB VRAM
# Recommended: 8192 for 8GB GPU, 32768 for 12GB+ GPU
OLLAMA_NUM_CTX=8192

# Vision Processing Settings
# Set to 'true' to skip AI image analysis (faster, uses less VRAM)
# The pipeline will still extract images but won't analyze them with AI
DISABLE_VISION_PROCESSING=false

# LLM Product Extraction - Maximum pages to scan
# Default: 50 (scans first 50 pages for product info)
# Set to 0 to scan ALL pages (slow but complete)
# Set to -1 to DISABLE LLM extraction (use pattern matching only)
# Why limit? LLM is slow (~15s/chunk). Large docs = long processing!
# Product specs are usually in first 50 pages anyway.
LLM_MAX_PAGES=50

# Maximum number of images to process with vision model per document
# Lower values = more stable, higher values = more complete analysis
# Recommended: 5-10 for 8GB VRAM, 10-20 for 12GB+ VRAM
MAX_VISION_IMAGES=5

# ===========================================
# GPU/CPU CONFIGURATION
# ===========================================
# Enable GPU acceleration for OpenCV and ML models
# Set to 'true' to use GPU (requires CUDA and opencv-contrib-python)
# Set to 'false' to use CPU only (default, works everywhere)
USE_GPU=false

# CUDA device to use (if USE_GPU=true)
# 0 = first GPU, 1 = second GPU, etc.
CUDA_VISIBLE_DEVICES=0
