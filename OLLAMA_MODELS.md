# ü§ñ Ben√∂tigte Ollama Modelle

## ‚úÖ **PFLICHT-MODELLE** (f√ºr Basis-Funktionalit√§t)

### 1Ô∏è‚É£ **embeddinggemma** - Vector Embeddings
```bash
ollama pull embeddinggemma
```

**Verwendung:**
- Generiert 768-dimensionale Embeddings
- Erm√∂glicht semantische Suche
- Stage 7: Embedding Processor

**Gr√∂√üe:** ~1.5 GB  
**Speed:** ~100 embeddings/Sekunde  
**RAM:** ~2 GB

---

### 2Ô∏è‚É£ **qwen2.5:7b** - LLM Extraktion
```bash
ollama pull qwen2.5:7b
```

**Verwendung:**
- Intelligente Produkt-Extraktion
- Spezifikations-Parsing
- JSON-strukturierte Antworten

**Gr√∂√üe:** ~4.7 GB  
**Speed:** Schnell  
**RAM:** ~6 GB

---

## üé® **OPTIONAL-MODELLE** (f√ºr Vision AI)

### 3Ô∏è‚É£ **llava-phi3** - Vision AI (EMPFOHLEN)
```bash
ollama pull llava-phi3
```

**Verwendung:**
- Bild-basierte Extraktion
- Tabellen-Erkennung
- Diagramm-Analyse

**Gr√∂√üe:** ~2.9 GB  
**Speed:** Schnell (Phi3-basiert)  
**RAM:** ~4 GB  
**Vorteil:** Schneller als llava:13b!

---

### 3Ô∏è‚É£ **ALTERNATIVE: llava:13b** - Vision AI (Besser, aber langsamer)
```bash
ollama pull llava:13b
```

**Verwendung:**
- Bessere Qualit√§t als llava-phi3
- Komplexe Tabellen

**Gr√∂√üe:** ~8 GB  
**Speed:** Langsamer  
**RAM:** ~12 GB

---

## üöÄ **SCHNELL-INSTALLATION**

### **Basis-Setup (ohne Vision AI):**
```bash
# Schritt 1: Ollama starten
ollama serve

# Schritt 2: Modelle installieren
ollama pull embeddinggemma
ollama pull qwen2.5:7b

# Schritt 3: Verifizieren
ollama list
```

**‚Üí Reicht f√ºr:**
- ‚úÖ Text-Extraktion
- ‚úÖ Produkt-Extraktion
- ‚úÖ Error Code-Extraktion
- ‚úÖ Link & Video-Extraktion
- ‚úÖ Embeddings
- ‚ùå Vision AI (Bilder/Tabellen)

---

### **FULL-Setup (mit Vision AI):**
```bash
# Basis-Modelle
ollama pull embeddinggemma
ollama pull qwen2.5:7b

# Vision AI (w√§hle EINES):
ollama pull llava-phi3        # SCHNELLER, empfohlen
# ODER
ollama pull llava:13b         # BESSER, aber langsamer

# Verifizieren
ollama list
```

**‚Üí Erm√∂glicht:**
- ‚úÖ Alles von Basis-Setup
- ‚úÖ Vision AI (OCR, Tabellen, Diagramme)
- ‚úÖ Image-basierte Produkt-Extraktion

---

## üìä **MODELL-VERGLEICH**

| Modell | Gr√∂√üe | RAM | Speed | Zweck |
|--------|-------|-----|-------|-------|
| **embeddinggemma** | 1.5 GB | 2 GB | ‚ö°‚ö°‚ö° | Embeddings (PFLICHT) |
| **qwen2.5:7b** | 4.7 GB | 6 GB | ‚ö°‚ö° | LLM Extraktion (PFLICHT) |
| **llava-phi3** | 2.9 GB | 4 GB | ‚ö°‚ö° | Vision AI (OPTIONAL) |
| **llava:13b** | 8 GB | 12 GB | ‚ö° | Vision AI besser (OPTIONAL) |

**Gesamt-Bedarf:**
- **Minimum:** 6.2 GB Disk + 8 GB RAM (ohne Vision)
- **Empfohlen:** 9.1 GB Disk + 12 GB RAM (mit llava-phi3)
- **Maximum:** 14.2 GB Disk + 20 GB RAM (mit llava:13b)

---

## ‚úÖ **VERIFIZIERUNG**

### **Nach Installation pr√ºfen:**
```bash
ollama list
```

**Sollte zeigen:**
```
NAME                 ID              SIZE      MODIFIED
embeddinggemma:latest   abc123...       1.5 GB    X minutes ago
qwen2.5:7b             def456...       4.7 GB    X minutes ago
llava-phi3:latest      ghi789...       2.9 GB    X minutes ago  (optional)
```

---

### **Test ob Ollama l√§uft:**
```bash
# Windows PowerShell
curl http://localhost:11434/api/tags

# Oder im Browser:
http://localhost:11434
```

---

## üîß **TROUBLESHOOTING**

### **Problem: "Ollama not found"**
```bash
# Windows:
winget install Ollama.Ollama

# Oder Download:
https://ollama.ai/download
```

---

### **Problem: "Model not found"**
```bash
# Liste alle installierten Modelle:
ollama list

# Modell neu installieren:
ollama pull embeddinggemma
```

---

### **Problem: "Out of memory"**
```bash
# Kleinere Vision-Modell verwenden:
ollama pull llava-phi3    # Statt llava:13b

# Oder Vision AI deaktivieren im Script:
enable_vision=False
```

---

### **Problem: "Ollama not responding"**
```bash
# Windows:
# 1. Task Manager ‚Üí Ollama beenden
# 2. Neu starten:
ollama serve

# Oder Service neu starten:
# Services ‚Üí Ollama ‚Üí Restart
```

---

## üìù **EMPFEHLUNG**

### **F√ºr normale Nutzung:**
```bash
ollama pull embeddinggemma
ollama pull qwen2.5:7b
ollama pull llava-phi3
```

**‚Üí 9.1 GB Disk, 12 GB RAM, beste Balance!**

---

### **F√ºr Low-RAM System (<16 GB):**
```bash
ollama pull embeddinggemma
ollama pull qwen2.5:7b
# Kein Vision AI Modell
```

**‚Üí 6.2 GB Disk, 8 GB RAM**

**Im Script dann setzen:**
```python
enable_vision=False
```

---

### **F√ºr High-End System (>32 GB RAM):**
```bash
ollama pull embeddinggemma
ollama pull qwen2.5:7b
ollama pull llava:13b    # Beste Qualit√§t
```

**‚Üí 14.2 GB Disk, 20 GB RAM, beste Qualit√§t!**

---

## üéØ **QUICK START**

```bash
# 1. Ollama starten (Neues Terminal)
ollama serve

# 2. Modelle installieren (Anderes Terminal)
ollama pull embeddinggemma
ollama pull qwen2.5:7b
ollama pull llava-phi3

# 3. Pr√ºfen
ollama list

# 4. Script starten
cd backend/processors_v2
python process_production.py
```

---

**Fertig! Alle ben√∂tigten Modelle sind installiert!** ‚úÖ
